{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IagZMs0_qjdL"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Welcome to your third assignment. In this assignment, you will build a deep neural network step by step. In this notebook, you will implement all the functions required to build a neural network.\n",
        "\n",
        "After finishing this assignment, you will have a deeper understanding of the process of training a deep neural network, which only consists of three steps: forward propagation, backward propagation and update."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BpSw0u6ZzuYu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGFR00CQvoaH"
      },
      "source": [
        "# 2. Packages\n",
        "All the packages that you need to finish this assignment are listed below.\n",
        "*   numpy : the fundamental package for scientific computing with Python.\n",
        "*   matplotlib : a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
        "*   math : Python has a built-in module that you can use for mathematical tasks.\n",
        "*   sklearn.datasets : scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. You will be using the breast cancer wisconsin dataset to build a binary classifier.\n",
        "\n",
        "⚠️ **WARNING** ⚠️: \n",
        "*   Please do not import any other packages.\n",
        "*   np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don't change the seed.\n",
        "\n",
        "❗ **Important** ❗: Please do not change the code outside this code bracket.\n",
        "```\n",
        "### START CODE HERE ### (≈ n lines of code)\n",
        "...\n",
        "### END CODE HERE ###\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmTH9UkeqdYf"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn import datasets\n",
        "\n",
        "output = {}"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Neural network\n",
        "In this section, you will need to implement a deep neural network from scratch all by yourself. If you are familiar with deep learning library, such as Tensorflow or PyTorch, it may seems easy for you. But if you don't, don't worry because we will guide you step by step. All you need to do is to follow the instructions and understand how each part works.\n",
        "\n",
        "As mentioned before, the process of training a deep neural network is composed of three steps: forward propagation, backward propagation, and update, so all the to-do in this section will be related to these three steps."
      ],
      "metadata": {
        "id": "w35ZkTwMc00G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0KHo8w9yqbY"
      },
      "source": [
        "class Dense():\n",
        "    def __init__(self, n_x, n_y, seed=1):\n",
        "        self.n_x = n_x\n",
        "        self.n_y = n_y\n",
        "        self.seed = seed\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Argument:\n",
        "        self.n_x -- size of the input layer\n",
        "        self.n_y -- size of the output layer\n",
        "        self.parameters -- python dictionary containing your parameters:\n",
        "                           W -- weight matrix of shape (n_y, n_x)\n",
        "                           b -- bias vector of shape (n_y, 1)\n",
        "        \"\"\"\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        # GRADED FUNCTION: linear_initialize_parameters\n",
        "        ### START CODE HERE ### (≈ 6 lines of code)\n",
        "        limit = math.sqrt(6 / (self.n_x + self.n_y))\n",
        "        W = np.random.rand(self.n_y,self.n_x)*(2*limit)+(-limit)\n",
        "        b = np.zeros((self.n_y,1))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        assert(W.shape == (self.n_y, self.n_x))\n",
        "        assert(b.shape == (self.n_y, 1))\n",
        "\n",
        "        self.parameters = {\"W\": W, \"b\": b}\n",
        "\n",
        "    def forward(self, A):\n",
        "        \"\"\"\n",
        "        Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "        Arguments:\n",
        "        A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "\n",
        "        Returns:\n",
        "        Z -- the input of the activation function, also called pre-activation parameter \n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: linear_forward\n",
        "        ### START CODE HERE ### (≈ 2 line of code)\n",
        "        Z = np.dot(self.parameters['W'],A)+self.parameters['b']\n",
        "        self.cache=(A, self.parameters['W'], self.parameters['b'])\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        assert(Z.shape == (self.parameters[\"W\"].shape[0], A.shape[1]))\n",
        "        \n",
        "        return Z\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "        Arguments:\n",
        "        dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "        self.dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "        self.db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "\n",
        "        \"\"\"\n",
        "        A_prev, W, b = self.cache\n",
        "        m = A_prev.shape[1]\n",
        "\n",
        "        # GRADED FUNCTION: linear_backward\n",
        "        ### START CODE HERE ### (≈ 3 lines of code)\n",
        "        self.dW = 1 / m * (np.dot(dZ,self.cache[0].T))\n",
        "        self.db = 1 / m * (np.sum(dZ,axis = 1,keepdims = True))\n",
        "        dA_prev = np.dot(self.cache[1].T,dZ)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        assert (dA_prev.shape == A_prev.shape)\n",
        "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
        "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
        "        \n",
        "        return dA_prev\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Update parameters using gradient descent\n",
        "        \n",
        "        Arguments:\n",
        "        learning rate -- step size\n",
        "        \"\"\"\n",
        "\n",
        "        # GRADED FUNCTION: linear_update_parameters\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
        "        self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
        "        ### END CODE HERE ###"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Implement a linear layer\n",
        "First, we will start by implementing one of the most commonly used layers in the deep neural network, called the dense layer. The dense layer is a linear layer applying a linear transformation to the incoming data:\n",
        "$Z = WA + b$, where $W$ and $b$ are the weight and bias.\n",
        "\n",
        "**Note**: Dense layers, also known as Fully-connected layers, connect every input neuron to every output neuron and are commonly used in neural networks.\n",
        "\n",
        "### 3.1.1. Initialize parameters\n",
        "**Exercise**: Create and initialize parameters of a linear layer using Glorot uniform initialization. (5%)\n",
        "\n",
        "**Instructions**:\n",
        "*   Use random initialization (uniform distribution) for the weight matrices. Draws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 / (fan_in + fan_out)) (fan_in is the number of input units in the weight tensor and fan_out is the number of output units).\n",
        "*   Use zero initialization for the biases."
      ],
      "metadata": {
        "id": "P_krGKUNg_Ix"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HNAWwmg8R7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2ef200-987c-4fd2-d7e6-efafe296f24b"
      },
      "source": [
        "dense = Dense(3, 1)\n",
        "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
        "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
        "\n",
        "dense = Dense(4, 1)\n",
        "output[\"linear_initialize_parameters\"] = dense.parameters"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W = [[-0.20325375  0.53968259 -1.22446471]]\n",
            "b = [[0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtPtH0j3BFN7"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W: </td>\n",
        "    <td>[[-0.20325375  0.53968259 -1.22446471]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b: </td>\n",
        "    <td>[[0.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abu7YqxeAeMz"
      },
      "source": [
        "### 3.1.2. Linear forward\n",
        "\n",
        "After initializing parameters, you will need to apply the linear transformation to the incoming data, and this can be simply done by matrix multiplication and addition.\n",
        "\n",
        "**Exercise**: Implement linear forward by applying the linear transformation. (5%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSf8JIyjaj_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43bf2c88-8dca-4ecd-8281-4f319c79bb95"
      },
      "source": [
        "A, W, b = np.array([[0, 0.5, 1], [1, 1.5, 2], [2, 2.5, 3]]), np.array([[0.1, 0.2, 0.3]]), np.array([[1.1]])\n",
        "dense = Dense(3, 1)\n",
        "dense.parameters = {\"W\": W, \"b\": b}\n",
        "Z = dense.forward(A)\n",
        "print(\"Z = \" + str(Z))\n",
        "\n",
        "A, W, b = np.array([[0, -0.5, -1], [1, 1.5, 2], [-2, -2.5, -3]]), np.array([[0.5, 0.3, 0.7]]), np.array([[-1.1]])\n",
        "dense = Dense(3, 1)\n",
        "dense.parameters = {\"W\": W, \"b\": b}\n",
        "Z = dense.forward(A)\n",
        "output[\"linear_forward\"] = (Z, dense.cache)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z = [[1.9 2.2 2.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpcPlE8-EUsR"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Z: </td>\n",
        "    <td>[[1.9 2.2 2.5]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.3. Linear backward\n",
        "Backpropagation is used to calculate the gradient of the loss function with respect to the parameters.\n",
        "\n",
        "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
        "\n",
        "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
        "\n",
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n",
        "\n",
        "**Exercise**: Use the 3 formulas above to implement `linear_backward()`. (5%)"
      ],
      "metadata": {
        "id": "-K8_obj6vIeT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg-PfP31NKH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a198d2e5-f9e7-4aec-8026-ad400d89e8c6"
      },
      "source": [
        "dZ, linear_cache = np.array([[1.5, 2.5], [0.5, 1.0]]), (np.array([[0.5, 1]]), np.array([[2.0], [1.0]]), np.array([[0.5], [1.0]]))\n",
        "dense = Dense(1, 2)\n",
        "dense.cache = linear_cache\n",
        "\n",
        "dA_prev = dense.backward(dZ)\n",
        "print (\"dA_prev = \" + str(dA_prev))\n",
        "print (\"dW = \" + str(dense.dW))\n",
        "print (\"db = \" + str(dense.db))\n",
        "\n",
        "dZ, linear_cache = np.array([[0.5, -1.5], [-1.5, 2.0]]), (np.array([[0.25, 1.25]]), np.array([[-1.0], [1.0]]), np.array([[-0.5], [-1.0]]))\n",
        "dense = Dense(1, 2)\n",
        "dense.cache = linear_cache\n",
        "dA_prev = dense.backward(dZ)\n",
        "output[\"linear_backward\"] = (dA_prev, dense.dW, dense.db)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dA_prev = [[3.5 6. ]]\n",
            "dW = [[1.625]\n",
            " [0.625]]\n",
            "db = [[2.  ]\n",
            " [0.75]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny0k-zxuNKIB"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[3.5 6. ]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[1.625]\n",
        " [0.625]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[2.  ]\n",
        " [0.75]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWNWxxutN47B"
      },
      "source": [
        "## 3.1.4. Linear update parameters\n",
        "In this section you will update the parameters of the linear layer, using gradient descent:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "\n",
        "**Exercise**: Implement update() to update your parameters using gradient descent. (5%)\n",
        "\n",
        "**Instructions**: \n",
        "*   Update parameters using gradient descent on $W^{[l]}$ and $b^{[l]}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMBqHniLN47I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96e3616-004e-4cad-d421-47b8bd7b07ba"
      },
      "source": [
        "np.random.seed(1)\n",
        "dense = Dense(1, 2)\n",
        "dense.parameters = {\"W\": np.array([[1.0], [2.0]]), \"b\": np.array([[0.5], [0.5]])}\n",
        "dense.dW = np.array([[0.5], [-0.5]])\n",
        "dense.db = np.array([[1.5], [-1.5]])\n",
        "dense.update(1.0)\n",
        "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
        "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
        "\n",
        "dense = Dense(3, 4)\n",
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(3,1), \"W2\": np.random.rand(1,3), \"b2\": np.random.rand(1,1)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(3,1), \"dW2\": np.random.rand(1,3), \"db2\": np.random.rand(1,1)}\n",
        "dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "dense.dW = grads[\"dW1\"]\n",
        "dense.db = grads[\"db1\"]\n",
        "dense.update(0.1)\n",
        "output[\"linear_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W = [[0.5]\n",
            " [2.5]]\n",
            "b = [[-1.]\n",
            " [ 2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIl13uvgN47I"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[0.5]\n",
        " [2.5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[-1.]\n",
        " [ 2.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syt1bV3bdI_f"
      },
      "source": [
        "## 3.2. Activation function layer\n",
        "\n",
        "In this section, you will need to implement activation function layers. There are many activation functions, such as sigmoid function, softmax function, ReLU function and etc. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnuv8MmebMgg"
      },
      "source": [
        "class Activation():\n",
        "    def __init__(self, function):\n",
        "        self.function = function\n",
        "\n",
        "    def forward(self, Z):\n",
        "        if self.function == \"sigmoid\":\n",
        "            \"\"\"\n",
        "            Implements the sigmoid activation in numpy\n",
        "            \n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "            \n",
        "            Returns:\n",
        "            A -- output of sigmoid(z), same shape as Z\n",
        "            \n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: sigmoid_forward\n",
        "            ### START CODE HERE ### (≈ 8 lines of code)\n",
        "            \n",
        "            A = 1/(1+np.exp(-Z))\n",
        "            self.cache = Z\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            return A\n",
        "\n",
        "        elif self.function == \"softmax\":\n",
        "            \"\"\"\n",
        "            Implements the softmax activation in numpy\n",
        "            \n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape (dim 0: number of classes, dim 1: number of samples)\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "            \n",
        "            Returns:\n",
        "            A -- output of softmax(z), same shape as Z\n",
        "            \"\"\"\n",
        "\n",
        "            # GRADED FUNCTION: softmax_forward\n",
        "            ### START CODE HERE ### (≈ 2 lines of code)\n",
        "            A = np.exp(Z)/np.sum(np.exp(Z),axis=0)\n",
        "            self.cache = Z\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            return A\n",
        "\n",
        "        elif self.function == \"relu\":\n",
        "            \"\"\"\n",
        "            Implement the RELU function in numpy\n",
        "            Arguments:\n",
        "            Z -- numpy array of any shape\n",
        "            self.cache -- stores Z as well, useful during backpropagation\n",
        "            Returns:\n",
        "            A -- output of relu(z), same shape as Z\n",
        "            \n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: relu_forward\n",
        "            ### START CODE HERE ### (≈ 2 lines of code)\n",
        "            A = np.maximum(0,Z)\n",
        "            self.cache = Z\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert(A.shape == Z.shape)\n",
        "            \n",
        "            return A\n",
        "\n",
        "    def backward(self, dA=None, Y=None):\n",
        "        if self.function == \"sigmoid\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a single SIGMOID unit.\n",
        "            Arguments:\n",
        "            dA -- post-activation gradient, of any shape\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the cost with respect to Z\n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: sigmoid_backward\n",
        "            ### START CODE HERE ### (≈ 9 lines of code)\n",
        "            Z = self.cache\n",
        "            s = 1/(1+np.exp(-Z))\n",
        "            dZ = dA * s * (1-s)\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert (dZ.shape == Z.shape)\n",
        "            \n",
        "            return dZ\n",
        "\n",
        "        elif self.function == \"relu\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a single RELU unit.\n",
        "            Arguments:\n",
        "            dA -- post-activation gradient, of any shape\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the cost with respect to Z\n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: relu_backward\n",
        "            ### START CODE HERE ### (≈ 3 lines of code)\n",
        "            Z = self.cache\n",
        "            dZ = np.array(dA, copy=True) # just converting dz to a correct object. \n",
        "            dZ[Z <= 0] = 0 # When z <= 0, you should set dz to 0 as well.\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert (dZ.shape == Z.shape)\n",
        "            \n",
        "            return dZ\n",
        "\n",
        "        elif self.function == \"softmax\":\n",
        "            \"\"\"\n",
        "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
        "            Arguments:\n",
        "            Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                                      in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "            Returns:\n",
        "            dZ -- Gradient of the cost with respect to Z\n",
        "            \"\"\"\n",
        "            \n",
        "            # GRADED FUNCTION: softmax_CCE_backward \n",
        "            ### START CODE HERE ### (≈ 3 lines of code)\n",
        "            Z = self.cache #三小\n",
        "            s = np.exp(Z)/np.sum(np.exp(Z),axis=0)\n",
        "            \n",
        "            dZ = s - Y\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            assert (dZ.shape == Z.shape)\n",
        "            \n",
        "            return dZ"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PkLKaFWiWmF"
      },
      "source": [
        "### 3.2.1. Activation forward\n",
        "#### 3.2.1.1. Sigmoid function\n",
        "Sigmoid: $\\sigma(Z) = \\begin{cases}\n",
        "    \\frac{1}{1+e^{-Z}},& \\text{if } Z >= 0\\\\\n",
        "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
        "\\end{cases}$. \n",
        "\n",
        "❗**Important**❗: As you can see, there is an exponential function inside the sigmoid function, so you might encounter an exponential overflow problem when implementing this function. To solve this problem, we use the numerically stable sigmoid function as shown in the equation above.\n",
        "\n",
        "### 3.2.1.2. Softmax function\n",
        "Softmax: $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{K} e^{Z_j-b}}$, where $\\vec{Z}$ = input vector, $K$ = number of classes in the multi-class classifier, $b$ is $\\max_{j=1}^{K} Z_j$\n",
        "\n",
        "❗**Important**❗: The naive implementation $\\sigma(\\vec{Z})_i = \\frac{e^{Z_i}}{\\sum_{j=1}^{K} e^{Z_j}}$ is terrible when there are large numbers! You might encounter the following problems if you use the naive implementation.\n",
        "*   RuntimeWarning: overflow encountered in exp\n",
        "\n",
        "\n",
        "### 3.2.1.3. ReLU (rectified linear unit) function\n",
        "ReLU: $RELU(Z) = max(Z, 0)$\n",
        "\n",
        "**Exercise**: Implement activation function. (5%+5%) (basic: Sigmoid and ReLU, advanced: Softmax)\n",
        "\n",
        "**Instruction**: \n",
        "*   Sigmoid: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function).\n",
        "*   Softmax: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function).\n",
        "*   ReLU: This function returns one item and stores one item: the activation value \"a\" and a cache contains \"z\" (it's what we will use in to the corresponding backward function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBuRAoeUC5jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0331003b-88dd-40f2-9c99-6fcb08ff1e8a"
      },
      "source": [
        "Z = np.array([[-5, -1, 0, 1, 5]])\n",
        "\n",
        "sigmoid = Activation(\"sigmoid\")\n",
        "A = sigmoid.forward(Z)\n",
        "print(\"Sigmoid: A = \" + str(A))\n",
        "A = sigmoid.forward(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
        "output[\"sigmoid\"] = (A, sigmoid.cache)\n",
        "\n",
        "relu = Activation(\"relu\")\n",
        "A = relu.forward(Z)\n",
        "print(\"ReLU: A = \" + str(A))\n",
        "A = relu.forward(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
        "output[\"relu\"] = (A, relu.cache)\n",
        "\n",
        "Z = np.array([[1, 0, -2], [2, 1, -1], [3, 0, 0], [4, 0, 1]])\n",
        "softmax = Activation(\"softmax\")\n",
        "A = softmax.forward(Z)\n",
        "print(\"Softmax: A = \\n\" + str(A))\n",
        "A = softmax.forward(np.array([[0.1, 1.2, -2.1], [2.2, 0.7, -1.3], [1.4, 0.3, 0.2], [3.9, 0.5, -1.6]]))\n",
        "output[\"softmax\"] = (A, softmax.cache) "
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid: A = [[0.00669285 0.26894142 0.5        0.73105858 0.99330715]]\n",
            "ReLU: A = [[0 0 0 1 5]]\n",
            "Softmax: A = \n",
            "[[0.0320586  0.1748777  0.0320586 ]\n",
            " [0.08714432 0.47536689 0.08714432]\n",
            " [0.23688282 0.1748777  0.23688282]\n",
            " [0.64391426 0.1748777  0.64391426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyyX_xxdEmNp"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) A: </td>\n",
        "    <td>[[0.00669285 0.26894142 0.5        0.73105858 0.99330715]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) A: </td>\n",
        "    <td>[[0 0 0 1 5]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) A: </td>\n",
        "    <td>[[0.0320586  0.1748777  0.0320586 ]\n",
        " [0.08714432 0.47536689 0.08714432]\n",
        " [0.23688282 0.1748777  0.23688282]\n",
        " [0.64391426 0.1748777  0.64391426]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tlaPl8PpcbE"
      },
      "source": [
        "### 3.2.2. Activation backward\n",
        "Next, you will need to implement the backward functions of `sigmoid()`, `relu()` and `softmax()`+`compute_CCE_cost`.\n",
        "\n",
        "**Exercise**: Implement backward function. (5%+5%) (basic: Sigmoid and ReLU, advanced: Softmax+CCE_loss)\n",
        "\n",
        "**Instruction**:\n",
        "*   sigmoid_backward: Implements the backward propagation for SIGMOID unit.\n",
        "*   relu_backward: Implements the backward propagation for RELU unit.\n",
        "*   softmax_CCE_backward: Implements the backward propagation for [SOFTMAX->CCE_LOSS] unit.\n",
        "\n",
        "If $g(.)$ is the activation function, sigmoid_backward, relu_backward and softmax_backward compute$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
        "\n",
        "1. The derivative of the sigmoid function is: $$σ^{'}(Z^{[l]}) = σ(Z^{[l]}) (1 - σ(Z^{[l]}))$$. <br>\n",
        "❗**Important**❗: You should use the numerically stable sigmoid function to prevent the overflow exponential problem. \n",
        "\n",
        "2. The derivative of the relu function is: $$g'(Z^{[l]}) = \\begin{cases}\n",
        "    1,& \\text{if } Z^{[l]}> 0\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "3. TLDR😉: The derivative of the categorical cross-entropy loss with respect to the last hidden layer is: $$\\frac{\\partial \\mathcal{L}}{\\partial Z} = s - y $$. <br> The derivative of the softmax function is: $$\\frac{\\partial S(z_i)}{\\partial z_j} = \\begin{cases}\n",
        "    S(z_i) \\times (1 - S(z_i)),& \\text{if } i = j\\\\\n",
        "    -S(z_i) \\times S(z_j),              & \\text{if } i \\neq j\n",
        "\\end{cases}$$, where $z$ is a vector with shape (number of classes K, 1) and $S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$. Hence, the real derivative of softmax function would be a full Jacobian matrix. For the special case, K = 4, we have <img src=\"https://miro.medium.com/max/554/1*SWfgFQLDIPXDf1C6CHmr8A.png\" height=\"100\"/>. <br> It is quite complicated to calculate the softmax derivative on its own. However, if you use the softmax and the cross entropy loss, that complexity fades away. Since the softmax layer is usually used at the output, we can actually calculate the derivative of the categorical cross-entropy loss with respect to the n-th node in the last hidden layer. Instead of a long clunky formula, you end up with this terse, easy to compute thing: $$\\frac{\\partial \\mathcal{L}}{\\partial Z_i} = s_i - y_i $$, where $s$ is the output of the softmax function and the $y$ is the true label vector(one-hot vector). For more information, you can refer to this article [Derivative of the Softmax Function and the Categorical Cross-Entropy Loss](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1). <br> \n",
        "❗**Important**❗: The above mathematical derivation is based on naive implementation. In order to deal with the exponential overflow problem, we should use the normalized exponential function when counting $s$. For the sake of simplicity, we just use the same gradient equation as the naive implementation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p1wxIeBpcbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c21e93-2f34-4a6b-88af-865e68d370d6"
      },
      "source": [
        "dA, cache = np.array([[-2, -1.37, -1.14, -2, -3.72]]), np.array([[0, 1, 2, 0, 1]])\n",
        "sigmoid = Activation(\"sigmoid\")\n",
        "sigmoid.cache = cache\n",
        "dZ = sigmoid.backward(dA=dA)\n",
        "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
        "dA, cache = np.array([[-2, -2, -1.37, -1.14, -3.72]]), np.array([[2, 0, 1.5, 0, 0.5]])\n",
        "sigmoid.cache = cache\n",
        "output[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n",
        "\n",
        "relu = Activation(\"relu\")\n",
        "dA, cache = np.array([[-2, -1.37, -1.14], [1.7, 2, 3.72]]), np.array([[-2, -1, 2], [1, 0, 1]])\n",
        "relu.cache = cache\n",
        "dZ = relu.backward(dA=dA)\n",
        "print(\"ReLU: dZ = \"+ str(dZ))\n",
        "dA, cache = np.array([[3.179, -1.376, -0.114], [2.227, -5.612, 4.172]]), np.array([[0.53, 1.21, -2.22], [-1.58, 0.99, -0.11]])\n",
        "relu.cache = cache\n",
        "output[\"relu_backward\"] = relu.backward(dA=dA)\n",
        "\n",
        "Y, cache = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([[-2, -1, -2], [1, 0, -2], [0, 1, 2]])\n",
        "softmax = Activation(\"softmax\")\n",
        "softmax.cache = cache\n",
        "dZ = softmax.backward(Y=Y)\n",
        "print(\"Softmax: dZ = \" + str(dZ))\n",
        "Y, cache = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([[-2.11, -1.22, -2.33], [1.44, 0.55, -2.66], [0.77, 1.88, 2.99]])\n",
        "softmax.cache = cache\n",
        "output[\"softmax_CCE_backward\"] = softmax.backward(Y=Y)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid: dZ = [[-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]\n",
            "ReLU: dZ = [[ 0.    0.   -1.14]\n",
            " [ 1.7   0.    3.72]]\n",
            "Softmax: dZ = [[-0.96488097  0.09003057  0.01766842]\n",
            " [ 0.70538451 -0.75527153  0.01766842]\n",
            " [ 0.25949646  0.66524096 -0.03533684]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwYDe3WfpcbF"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) dZ: </td>\n",
        "    <td>[[-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) dZ: </td>\n",
        "    <td>[[ 0.    0.   -1.14]\n",
        " [ 1.7   0.    3.72]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) dZ: </td>\n",
        "    <td>[[-0.96488097  0.09003057  0.01766842]\n",
        " [ 0.70538451 -0.75527153  0.01766842]\n",
        " [ 0.25949646  0.66524096 -0.03533684]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Model\n",
        "Alright, now you have all the tools that are needed to build a model. Let's get started! 😀\n",
        "\n",
        "### 3.3.1. Model initialize parameters\n",
        "First, you will need to initialize your model by creating several linear and activation function layers. \n",
        "\n",
        "**Exercise**: Implement model initialize parameters. (5%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Store all the linear layers in a list called linear.\n",
        "*   Store all the activation function layers in a list called activation.\n",
        "\n",
        "❗**Important**❗: We set the random seed for grading purposes to keep all the random function calls consistent. However, we still want all the linear layers to have different initialized weights, so when implementing this function, please make sure that you pass the number of iterations as the seed number to the Dense layer initialization call.\n",
        "\n",
        "**Note**: In deep learning, a linear-activation layer is counted as a single layer in the neural network, not two layers since the activation layer does not have any parameter."
      ],
      "metadata": {
        "id": "RYqpQu6Eye7h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JGMzfIDCSVz"
      },
      "source": [
        "class Model():\n",
        "    def __init__(self, units, activation_functions):\n",
        "        self.units = units\n",
        "        self.activation_functions = activation_functions\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        self.units -- number of nodes/units for each layer, starting from the input dimension and ending with the output dimension (i.e., [4, 4, 1])\n",
        "        self.activation_functions -- activation functions used in each layer (i.e, [\"relu\", \"sigmoid\"])\n",
        "        self.linear -- a list to store the dense layers when initializing the model\n",
        "        self.activation -- a list to store the activation function layers when initializing the model\n",
        "        \"\"\"\n",
        "        self.linear = []\n",
        "        self.activation = []\n",
        "\n",
        "        # GRADED FUNCTION: model_initialize_parameters\n",
        "        ### START CODE HERE ### (≈ 5 lines of code)\n",
        "        \n",
        "        for i in range(1,len(self.units)):\n",
        "          self.linear.append(Dense(self.units[i-1], self.units[i],i-1))\n",
        "        for i in self.activation_functions:\n",
        "          self.activation.append(Activation(i))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        X -- input data: (number of features, number of examples)\n",
        "        \n",
        "        Returns:\n",
        "        A -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "        \"\"\"\n",
        "        A = X\n",
        "\n",
        "        # GRADED FUNCTION: model_forward\n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        for l in range(0,len(self.units)-1):\n",
        "          A_prev=A\n",
        "          A=self.activation[l].forward(self.linear[l].forward(A_prev))\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, AL=None, Y=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        For multi-class classification,\n",
        "        AL -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "        Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                              in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "\n",
        "        Returns:\n",
        "        dA_prev -- post-activation gradient\n",
        "        \"\"\"\n",
        "\n",
        "        L = len(self.linear)\n",
        "\n",
        "        # GRADED FUNCTION: model_backward\n",
        "        ### START CODE HERE ### (≈ 10 lines of code)\n",
        "        #Y = Y.reshape(AL.shape)\n",
        "        if self.activation_functions[-1] == \"sigmoid\":\n",
        "            # Initializing the backpropagation\n",
        "            dAL = - (np.divide(Y, AL + 1e-5) - np.divide(1 - Y, 1 - AL + 1e-5))\n",
        "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
        "            dZ = self.activation[-1].backward(dAL, Y)\n",
        "            dA_prev = self.linear[-1].backward(dZ)\n",
        "        else:\n",
        "            # # Initializing the backpropagation\n",
        "            # dZ = None\n",
        "\n",
        "            # # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
        "            # dA_prev = None\n",
        "            dAL = - (np.divide(Y, AL + 1e-5) - np.divide(1 - Y, 1 - AL + 1e-5))\n",
        "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
        "            dZ = self.activation[-1].backward(dAL, Y)\n",
        "            dA_prev = self.linear[-1].backward(dZ)\n",
        "\n",
        "        # Loop from l=L-2 to l=0\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
        "        for l in reversed(range(L-1)):\n",
        "          dZ = self.activation[l].backward(dA_prev, Y)\n",
        "          dA_prev = self.linear[l].backward(dZ)\n",
        "\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "        learning_rate -- step size\n",
        "        \"\"\"\n",
        "\n",
        "        L = len(self.linear)\n",
        "\n",
        "        # GRADED FUNCTION: model_update_parameters\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        for l in range(L):\n",
        "          self.linear[l].update(learning_rate)\n",
        "        ### END CODE HERE ###"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n",
        "print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \", model.linear[1].parameters[\"b\"])\n",
        "\n",
        "model = Model([16, 8, 1], [\"relu\", \"sigmoid\"])\n",
        "output[\"model_initialize_parameters\"] = (model.linear[0].parameters, model.linear[1].parameters)"
      ],
      "metadata": {
        "id": "EGY7_1bjcm-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3878e1eb-f73f-4cea-f8db-c2cd86bd45ae"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:  [[ 0.09762701  0.43037873  0.20552675]\n",
            " [ 0.08976637 -0.1526904   0.29178823]\n",
            " [-0.12482558  0.783546    0.92732552]] \n",
            "b1:  [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2:  [[-0.20325375  0.53968259 -1.22446471]] \n",
            "b2:  [[0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[ 0.09762701  0.43037873  0.20552675]\n",
        " [ 0.08976637 -0.1526904   0.29178823]\n",
        " [-0.12482558  0.783546    0.92732552]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[0.]\n",
        " [0.]\n",
        " [0.]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2: </td>\n",
        "    <td>[[-0.20325375  0.53968259 -1.22446471]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2: </td>\n",
        "    <td>[[0.]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "LEmggOxtdMnl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJVlZeyNAu-y"
      },
      "source": [
        "### 3.3.2. Model forward\n",
        "\n",
        "After that, you will implement the model forward function by calling the forward function of each layer in the linear and activation function layer you have created in the previous step.\n",
        "\n",
        "For a $N$-layer neural network, you will call the forward function of the linear layers and then followed by the activation function layers for $N-1$ times. The last activation function layer will be sigmoid for binary classification and softmax for multi-class classification.\n",
        "\n",
        "**Exercise**: Implement model forward. (5%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Use a for loop to replicate [LINEAR->ACTIVATION] (N-1) times.\n",
        "\n",
        "**Note**: There are K nodes in the last layer for K-class classification, but only one node for binary classification. Intuitively, this could be pretty confusing sometimes since there should be two nodes in the last layer for binary classification. However, both the one-node(sigmoid, binary cross-entropy) and two-node(softmax, categorical cross-entropy) techniques for binary classification work fine, and picking one technique over the other is a matter of subjective preference. For this assignment, you will implement the former one, which is what we usually do for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yVQQqe2EyHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7127642e-c03b-4ec5-d39f-ec56d01f1856"
      },
      "source": [
        "A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3]]), np.array([[-0.5]])\n",
        "model = Model([3, 1], [\"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "A_prev, W, b = np.array([[1.1, -2.2], [-3.9, 0.6]]), np.array([[9.1, -8.2]]), np.array([[0.5]])\n",
        "model = Model([2, 1], [\"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "output[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
        "\n",
        "A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3]]), np.array([[-0.5]])\n",
        "model = Model([3, 1], [\"relu\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With ReLU: A = \" + str(A))\n",
        "A_prev, W, b = np.array([[1.1, -2.2], [-3.9, 0.6]]), np.array([[9.1, -8.2]]), np.array([[0.5]])\n",
        "model = Model([2, 1], [\"relu\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "output[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
        "\n",
        "A_prev, W, b = np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]), np.array([[0.1, 0.2, 0.3], [-0.1, -0.2, -0.3], [-0.1, 0, 0.1]]), np.array([[-0.5], [0.5], [0.1]])\n",
        "model = Model([3, 3], [\"softmax\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "print(\"With softmax: A = \\n\" + str(A))\n",
        "A_prev, W, b = np.array([[-0.1, 1.2, 1.9], [-1.1, 0.2, -2.3], [2.9, -2.5, -3.7]]), np.array([[0.2, 0.2, 0.2], [-0.1, -0.1, -0.1], [-0.1, 0, 0.1]]), np.array([[-0.1], [0.1], [0.5]])\n",
        "model = Model([3, 3], [\"softmax\"])\n",
        "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
        "A = model.forward(A_prev)\n",
        "output[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With sigmoid: A = [[0.64565631 0.20915937 0.77902611]]\n",
            "With ReLU: A = [[0.6  0.   1.26]]\n",
            "With softmax: A = \n",
            "[[0.47535001 0.05272708 0.68692136]\n",
            " [0.14317267 0.75380161 0.05526942]\n",
            " [0.38147732 0.19347131 0.25780921]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMkf2ss6F52W"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(With sigmoid) A: </td>\n",
        "    <td>[[0.64565631 0.20915937 0.77902611]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With ReLU) A: </td>\n",
        "    <td>[[0.6  0.   1.26]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(With softmax) A: </td>\n",
        "    <td>[[0.47535001 0.05272708 0.68692136]\n",
        " [0.14317267 0.75380161 0.05526942]\n",
        " [0.38147732 0.19347131 0.25780921]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s26LVkCbIbJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80d362a-a772-44f3-bc5f-e2e83c2fe6bd"
      },
      "source": [
        "# binary classification\n",
        "X = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]])\n",
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of layers list = \" + str(len(model.linear)))\n",
        "\n",
        "# multi-class classification\n",
        "X = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]])\n",
        "model = Model([3, 3, 10], [\"relu\", \"softmax\"])\n",
        "AL = model.forward(X)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of layers list = \" + str(len(model.linear)))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AL = [[0.56058713 0.55220559 0.46331713]]\n",
            "Length of layers list = 2\n",
            "AL = [[0.11637212 0.11413265 0.09750771]\n",
            " [0.08186754 0.08432761 0.07419482]\n",
            " [0.0924809  0.09365443 0.08444682]\n",
            " [0.09675205 0.09736489 0.10943351]\n",
            " [0.12819411 0.12404237 0.09669465]\n",
            " [0.09664001 0.09726785 0.11116299]\n",
            " [0.08448599 0.08664355 0.08734059]\n",
            " [0.09067641 0.09207969 0.12452515]\n",
            " [0.1294968  0.12512634 0.13002144]\n",
            " [0.08303407 0.08536063 0.08467232]]\n",
            "Length of layers list = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoCdrONOHhvw"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>(Binary classification) AL: </td>\n",
        "    <td>[[0.56058713 0.55220559 0.46331713]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Binary classification) Length of layers list: </td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Multi-class classification) AL: </td>\n",
        "    <td>[[0.11637212 0.11413265 0.09750771]\n",
        " [0.08186754 0.08432761 0.07419482]\n",
        " [0.0924809  0.09365443 0.08444682]\n",
        " [0.09675205 0.09736489 0.10943351]\n",
        " [0.12819411 0.12404237 0.09669465]\n",
        " [0.09664001 0.09726785 0.11116299]\n",
        " [0.08448599 0.08664355 0.08734059]\n",
        " [0.09067641 0.09207969 0.12452515]\n",
        " [0.1294968  0.12512634 0.13002144]\n",
        " [0.08303407 0.08536063 0.08467232]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(Multi-class classification) Length of layers list: </td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPBl7iq7N2wY"
      },
      "source": [
        "###3.3.3. Model backward\n",
        "Now you will implement the backward function for the whole network. Recall that you have implemented the backward function for the dense and activation function layer. In this section, you will call these functions to help you implement the model backward function. You will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will call the backward function of layer $l$ to backpropagate through layer $l$.\n",
        "\n",
        "**Exercise**: Implement model backward. (5%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Initialize backpropagation.\n",
        "*   Use a for loop to backprop from layer $L-1$ to layer $1$.\n",
        "\n",
        "Initializing backpropagation:\n",
        "\n",
        "(1) Binary classification: To backpropagate through this network, we know that the output is, $A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute dAL $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$. To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
        "```\n",
        "dAL = - (np.divide(Y, AL + ϵ) - np.divide(1 - Y, 1 - AL + ϵ)) # derivative of cost with respect to AL, where ϵ = 1e-5 is added to prevent zero division.\n",
        "```\n",
        "\n",
        "You can then use this post-activation gradient dAL to keep going backward. You can now feed in dAL into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored inside each layer in the forward pass). After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function. \n",
        "\n",
        "(2) Multi-class classification: Since you have implemented the backward function of the softmax activation function layer along with the categorical cross-entropy loss, you can directly call the softmax_CCE_backward function implemented inside the activation function layer and followed by the linear backward function to obtain the post-activation gradient to keep going backward. After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOGsyLXPNGh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b78026e-0ab1-4c6b-e2f3-77c66c3dc7c1"
      },
      "source": [
        "AL, Y, linear_activation_cache  = np.array([[0.1, 0.2, 0.5, 0.9, 1.0]]), np.array([[0, 0, 1, 1, 1]]), ((np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[2.0, 1.0]]), np.array([[0.5]])), np.array([[0, 1, 2, 0, 1]]))\n",
        "model = Model([2, 1], [\"sigmoid\"])\n",
        "model.linear[0].cache = linear_activation_cache[0]\n",
        "model.activation[0].cache = linear_activation_cache[1]\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(model.linear[0].dW))\n",
        "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
        "\n",
        "AL, Y, linear_activation_cache  = np.array([[0.15, 0.23, 0.79, 0.97, 0.99]]), np.array([[0, 0, 1, 1, 1]]), ((np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[2.0, 1.0]]), np.array([[0.5]])), np.array([[0, 1, 2, 0, 1]]))\n",
        "model = Model([2, 1], [\"sigmoid\"])\n",
        "model.linear[0].cache = linear_activation_cache[0]\n",
        "model.activation[0].cache = linear_activation_cache[1]\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "output[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db) \n",
        "\n",
        "X, Y = np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[0, 1, 1, 1, 1]])\n",
        "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(model.linear[0].dW))\n",
        "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
        "\n",
        "X, Y = np.array([[-2.5, -1.3, 0.1, 1.9, 2.7], [1.2, 2.1, 3.0, -4.1, -5.2]]), np.array([[1, 1, 0, 0, 0]])\n",
        "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "output[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid:\n",
            "dA_prev = [[ 0.55554938  0.49152369 -0.41996594 -0.55554938 -0.39321993]\n",
            " [ 0.27777469  0.24576184 -0.20998297 -0.27777469 -0.19660997]]\n",
            "dW = [[-0.29446117  0.29446117]]\n",
            "db = [[-0.03216622]]\n",
            "\n",
            "relu:\n",
            "dA_prev = [[-0.01269296  0.01470136  0.         -0.07496777 -0.07151883]\n",
            " [-0.05595562  0.06480946  0.         -0.0327431  -0.03123674]]\n",
            "dW = [[ 0.0178719  -0.0178719 ]\n",
            " [-0.17321413  0.17321413]]\n",
            "db = [[ 0.00335943]\n",
            " [-0.11638953]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xzEk3-NGh6"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Sigmoid </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[ 0.55554938  0.49152369 -0.41996594 -0.55554938 -0.39321993]\n",
        " [ 0.27777469  0.24576184 -0.20998297 -0.27777469 -0.19660997]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[-0.29446117  0.29446117]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[-0.03216622]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>ReLU </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA_prev: </td>\n",
        "    <td>[[-0.01269296  0.01470136  0.         -0.07496777 -0.07151883]\n",
        " [-0.05595562  0.06480946  0.         -0.0327431  -0.03123674]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW: </td>\n",
        "    <td>[[ 0.0178719  -0.0178719 ]\n",
        " [-0.17321413  0.17321413]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db: </td>\n",
        "    <td>[[ 0.00335943]\n",
        " [-0.11638953]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1QnMSKN2wZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a14ad5-6569-4964-9ac8-0b6121cdec53"
      },
      "source": [
        "# binary classification\n",
        "X, Y = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), np.array([[1, 0, 0]])\n",
        "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "AL = model.forward(X)\n",
        "\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print(\"Binary classification\")\n",
        "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
        "print(\"db1 = \"+ str(model.linear[0].db))\n",
        "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
        "\n",
        "# multi-class classification\n",
        "X, Y= np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "model = Model([3, 3, 3], [\"relu\", \"softmax\"])\n",
        "AL = model.forward(X)\n",
        "dA_prev = model.backward(AL=AL, Y=Y)\n",
        "print(\"Multi-class classification\")\n",
        "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
        "print(\"db1 = \"+ str(model.linear[0].db))\n",
        "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary classification\n",
            "dW1 = [[-0.06277946  0.         -0.01569486]\n",
            " [ 0.26602938  0.05875647  0.05181823]\n",
            " [-0.37820327  0.         -0.09455082]]\n",
            "db1 = [[-0.03138973]\n",
            " [ 0.10363646]\n",
            " [-0.18910163]]\n",
            "dA_prev = [[-0.02128713  0.02675119  0.08406585]\n",
            " [ 0.03620889 -0.04550313 -0.52321654]\n",
            " [-0.06919444  0.08695554 -0.47247201]]\n",
            "\n",
            "Multi-class classification\n",
            "dW1 = [[ 0.16593371  0.          0.04148343]\n",
            " [ 0.33171007  0.15006987  0.04541005]\n",
            " [-0.32297709  0.         -0.08074427]]\n",
            "db1 = [[ 0.08296685]\n",
            " [ 0.0908201 ]\n",
            " [-0.16148854]]\n",
            "dA_prev = [[-0.04735391  0.05429414  0.10229066]\n",
            " [ 0.08054785 -0.09235301 -0.30227651]\n",
            " [-0.15392528  0.1764847  -0.34116033]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cYzCzY8N2wZ"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Binary classification </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1: </td>\n",
        "    <td>[[-0.06277946  0.         -0.01569486]\n",
        " [ 0.26602938  0.05875647  0.05181823]\n",
        " [-0.37820327  0.         -0.09455082]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1: </td>\n",
        "    <td>[[-0.03138973]\n",
        " [ 0.10363646]\n",
        " [-0.18910163]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA1: </td>\n",
        "    <td>[[-0.02128713  0.02675119  0.08406585]\n",
        " [ 0.03620889 -0.04550313 -0.52321654]\n",
        " [-0.06919444  0.08695554 -0.47247201]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Multi-class classification </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW1: </td>\n",
        "    <td>[[ 0.16593371  0.          0.04148343]\n",
        " [ 0.33171007  0.15006987  0.04541005]\n",
        " [-0.32297709  0.         -0.08074427]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1: </td>\n",
        "    <td>[[ 0.08296685]\n",
        " [ 0.0908201 ]\n",
        " [-0.16148854]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dA1: </td>\n",
        "    <td>[[-0.04735391  0.05429414  0.10229066]\n",
        " [ 0.08054785 -0.09235301 -0.30227651]\n",
        " [-0.15392528  0.1764847  -0.34116033]]</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.4. Model update parameters\n",
        "In this section you will update the parameters of the model, using gradient descent:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "**Exercise**: Implement update() to update your parameters using gradient descent. (5%)\n",
        "\n",
        "**Instructions**: \n",
        "*   Use the functions you had previously written.\n",
        "*   Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$.\n"
      ],
      "metadata": {
        "id": "5wiJu3YlUCc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(3,1), \"W2\": np.random.rand(1,3), \"b2\": np.random.rand(1,1)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(3,1), \"dW2\": np.random.rand(1,3), \"db2\": np.random.rand(1,1)}\n",
        "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
        "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
        "model.update(0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n",
        "print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n",
        "print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n",
        "print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n",
        "\n",
        "np.random.seed(1)\n",
        "parameters, grads = {\"W1\": np.random.randn(3, 4), \"b1\": np.random.randn(3,1), \"W2\": np.random.randn(1,3), \"b2\": np.random.randn(1,1)}, {\"dW1\": np.random.randn(3, 4), \"db1\": np.random.randn(3,1), \"dW2\": np.random.randn(1,3), \"db2\": np.random.randn(1,1)}\n",
        "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"])\n",
        "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
        "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
        "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
        "model.update(0.075)\n",
        "output[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"
      ],
      "metadata": {
        "id": "qoGA4O8BUCvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63cbfbe-033f-4393-e430-d6bcdfd16049"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[ 0.39721186  0.64025004 -0.09671178  0.27099015]\n",
            " [ 0.07752363  0.00469968  0.09679955  0.33705631]\n",
            " [ 0.392862    0.52183369  0.33138026  0.67538482]]\n",
            "b1 = [[ 0.16234149]\n",
            " [ 0.78232848]\n",
            " [-0.02592894]]\n",
            "W2 = [[0.6012798  0.38575324 0.49003974]]\n",
            "b2 = [[0.05692437]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>W1: </td>\n",
        "    <td>[[ 0.39721186  0.64025004 -0.09671178  0.27099015]\n",
        " [ 0.07752363  0.00469968  0.09679955  0.33705631]\n",
        " [ 0.392862    0.52183369  0.33138026  0.67538482]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1: </td>\n",
        "    <td>[[ 0.16234149]\n",
        " [ 0.78232848]\n",
        " [-0.02592894]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2: </td>\n",
        "    <td>[[0.6012798  0.38575324 0.49003974]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2: </td>\n",
        "    <td>[[0.05692437]]</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "9t-HfnHZWYIa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmSBVaQOSRrk"
      },
      "source": [
        "# 4. Cost function\n",
        "In this section, you will implement the cost function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification. You need to compute the cost, because you want to check if your model is actually learning. Cross-entropy loss is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log loss of 0.0.\n",
        "\n",
        "## 4.1. Binary cross-entropy loss\n",
        "**Exercise**: Compute the binary cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+ϵ\\right)), where\\ ϵ=1e-5$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjBT0eYQaY81"
      },
      "source": [
        "# GRADED FUNCTION: compute_BCE_cost\n",
        "\n",
        "def compute_BCE_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the binary cross-entropy cost function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- binary cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    cost = -1./m * np.sum(Y*np.log(AL+1e-5)+(1-Y)*np.log(1-AL+1e-5))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r07sqnIXaaMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9945b50b-e80d-41cf-8674-e7006979d65e"
      },
      "source": [
        "AL, Y = np.array([[0.9, 0.6, 0.4, 0.1, 0.2, 0.8]]), np.array([[1, 1, 1, 0, 0, 0]])\n",
        "\n",
        "print(\"cost = \" + str(compute_BCE_cost(AL, Y)))\n",
        "output[\"compute_BCE_cost\"] = compute_BCE_cost(np.array([[0.791, 0.983, 0.654, 0.102, 0.212, 0.091, 0.476, 0.899]]), np.array([[1, 1, 1, 1, 0, 0, 0, 0]]))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = 0.5783820772863568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iRtgOx_IGPo"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>cost: </td>\n",
        "    <td>0.5783820772863568</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aealRyKbcQzG"
      },
      "source": [
        "## 4.2. Categorical cross-entropy loss\n",
        "**Exercise**: Compute the categorical cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}+ϵ\\right)), where\\ ϵ = 1e-5$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owx-kTdcfxV5"
      },
      "source": [
        "# GRADED FUNCTION: compute_CCE_cost\n",
        "\n",
        "def compute_CCE_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the categorical cross-entropy cost function using the above formula.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "    Y -- true \"label\" vector (one hot vector, for example: [[1], [0], [0]] represents rock, [[0], [1], [0]] represents paper, [[0], [0], [1]] represents scissors \n",
        "                              in a Rock-Paper-Scissors image classification), shape (number of classes, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- categorical cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    cost = -1./m * np.sum(Y*np.log(AL+1e-5))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YbHVAc7hSh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd87e171-2cc6-4f40-a760-61f96f51c6d9"
      },
      "source": [
        "AL, Y = np.array([[0.8, 0.6, 0.4, 0.1, 0.2, 0.4], [0.1, 0.3, 0.5, 0.7, 0.1, 0.1], [0.1, 0.1, 0.1, 0.2, 0.7, 0.5]]), np.array([[1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1]])\n",
        "print(\"cost = \" + str(compute_CCE_cost(AL, Y)))\n",
        "output[\"compute_CCE_cost\"] = compute_CCE_cost(np.array([[0.711, 0.001, 0.11], [0.099, 0.217, 0.09], [0.035, 0.599, 0.12], [0.068, 0.123, 0.1], [0.087, 0.06, 0.58]]), np.array([[1, 0, 0], [0, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1]]))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = 0.4722526144672341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9VVIBB5Ic-D"
      },
      "source": [
        "Expected output: \n",
        "<table>\n",
        "  <tr>\n",
        "    <td>cost: </td>\n",
        "    <td>0.4722526144672341</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpQah0JDdMyl"
      },
      "source": [
        "# Basic implementation (binary classification)\n",
        "\n",
        "Congratulations on implementing all the functions by yourself. You have done an incredible job! 👏\n",
        "\n",
        "Now you have all the tools you need to get started with classification. In this section, you will build a binary classifier using the functions you had previously written. You will create a model that can determine whether breast cancer is malignant or benign based on 30 features. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe the characteristics of the cell nuclei present in the image.\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus:\n",
        "\n",
        "1.   radius (mean of distances from center to points on the perimeter)\n",
        "2.   texture (standard deviation of gray-scale values)\n",
        "3.   perimeter\n",
        "4.   area\n",
        "5.   smoothness (local variation in radius lengths)\n",
        "6.   compactness (perimeter^2 / area - 1.0)\n",
        "7.   concavity (severity of concave portions of the contour)\n",
        "8.   concave points (number of concave portions of the contour)\n",
        "9.   symmetry\n",
        "10.   fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three\n",
        "largest values) of these features were computed for each image,\n",
        "resulting in 30 features. For instance, field 3 is Mean Radius, field\n",
        "13 is Radius SE, field 23 is Worst Radius.\n",
        "\n",
        "**Exercise**: Implement a binary classifier and tune hyperparameter. (10%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Preprocess the data by using min-max scaling to normalize X. Normalize the values of each feature between 0 and 1.\n",
        "*   Use batch gradient descent to train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load breast cancer wisconsin dataset\n",
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "X = X[:500].T\n",
        "y = np.expand_dims(y[:500], axis=1).T\n",
        "\n",
        "print(\"shape of X: \" + str(X.shape))\n",
        "print(\"shape of y: \" + str(y.shape))\n",
        "\n",
        "# GRADED CODE: binary classification\n",
        "### START CODE HERE ###\n",
        "# min max scaling\n",
        "\n",
        "X[:,:] = (X[:,:] - X[:,:].min()) / (X[:,:].max() - X[:,:].min())\n",
        "print(X)\n",
        "### END CODE HERE ###\n",
        "\n",
        "# split training set and validation set\n",
        "X_train, y_train = X[:, :400], y[:, :400]\n",
        "X_val, y_val = X[:, 400:], y[:, 400:]\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape) + \" shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_val: \" + str(X_val.shape) + \" shape of y_val: \" + str(y_val.shape))"
      ],
      "metadata": {
        "id": "5OzSS4zFHezi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6ccb99-7f55-4a1a-d50c-bcc99bfe517f"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X: (30, 500)\n",
            "shape of y: (1, 500)\n",
            "[[4.22896098e-03 4.83544899e-03 4.62858486e-03 ... 2.93135872e-03\n",
            "  4.34649741e-03 4.84015045e-03]\n",
            " [2.44005642e-03 4.17724495e-03 4.99529854e-03 ... 4.06911142e-03\n",
            "  4.11847673e-03 4.99294781e-03]\n",
            " [2.88669488e-02 3.12411848e-02 3.05594734e-02 ... 1.89116126e-02\n",
            "  2.85143394e-02 3.23930418e-02]\n",
            " ...\n",
            " [6.23883404e-05 4.37235543e-05 5.71227080e-05 ... 2.47531735e-05\n",
            "  3.90926187e-05 4.96708980e-05]\n",
            " [1.08157029e-04 6.46450400e-05 8.49318289e-05 ... 7.13446168e-05\n",
            "  5.90032910e-05 5.82980724e-05]\n",
            " [2.79501646e-05 2.09261871e-05 2.05876822e-05 ... 1.80089328e-05\n",
            "  2.22026328e-05 2.11542078e-05]]\n",
            "shape of X_train: (30, 400) shape of y_train: (1, 400)\n",
            "shape of X_val: (30, 100) shape of y_val: (1, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI7JY5ESjhZ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70e51dd2-d0c6-4b8d-e687-a896737e945c"
      },
      "source": [
        "# GRADED CODE: binary classification\n",
        "### START CODE HERE ###\n",
        "layers_dims = [30,20,4,1]\n",
        "activation_fn = [\"relu\",\"relu\", \"sigmoid\"]\n",
        "learning_rate = 0.3\n",
        "num_iterations = 125000\n",
        "print_cost = True\n",
        "classes = 2\n",
        "costs = []                         # keep track of cost\n",
        "model = Model(layers_dims, activation_fn)\n",
        "\n",
        "# Loop (batch gradient descent)\n",
        "for i in range(0, num_iterations):\n",
        "    # forward\n",
        "    AL = model.forward(X_train)\n",
        "\n",
        "    # compute cost\n",
        "    if classes == 2:\n",
        "        cost = compute_BCE_cost(AL, y_train)\n",
        "    else:\n",
        "        cost = compute_CCE_cost(AL, y_train)\n",
        "\n",
        "    # backward\n",
        "    dA_prev = model.backward(AL,y_train)\n",
        "    \n",
        "    # update\n",
        "    model.update(learning_rate)\n",
        "\n",
        "    if print_cost and i % 100 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        costs.append(cost)\n",
        "            \n",
        "# plot the cost\n",
        "plt.plot(np.squeeze(costs))\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.693104\n",
            "Cost after iteration 100: 0.426320\n",
            "Cost after iteration 200: 0.298347\n",
            "Cost after iteration 300: 0.251112\n",
            "Cost after iteration 400: 0.248384\n",
            "Cost after iteration 500: 0.242395\n",
            "Cost after iteration 600: 0.241475\n",
            "Cost after iteration 700: 0.226679\n",
            "Cost after iteration 800: 0.226848\n",
            "Cost after iteration 900: 0.230072\n",
            "Cost after iteration 1000: 0.228747\n",
            "Cost after iteration 1100: 0.225507\n",
            "Cost after iteration 1200: 0.223482\n",
            "Cost after iteration 1300: 0.221502\n",
            "Cost after iteration 1400: 0.220404\n",
            "Cost after iteration 1500: 0.219467\n",
            "Cost after iteration 1600: 0.218162\n",
            "Cost after iteration 1700: 0.217681\n",
            "Cost after iteration 1800: 0.216568\n",
            "Cost after iteration 1900: 0.215692\n",
            "Cost after iteration 2000: 0.214885\n",
            "Cost after iteration 2100: 0.214041\n",
            "Cost after iteration 2200: 0.213298\n",
            "Cost after iteration 2300: 0.212495\n",
            "Cost after iteration 2400: 0.211721\n",
            "Cost after iteration 2500: 0.211082\n",
            "Cost after iteration 2600: 0.208086\n",
            "Cost after iteration 2700: 0.209447\n",
            "Cost after iteration 2800: 0.208757\n",
            "Cost after iteration 2900: 0.208042\n",
            "Cost after iteration 3000: 0.207336\n",
            "Cost after iteration 3100: 0.206737\n",
            "Cost after iteration 3200: 0.206018\n",
            "Cost after iteration 3300: 0.205460\n",
            "Cost after iteration 3400: 0.204656\n",
            "Cost after iteration 3500: 0.203963\n",
            "Cost after iteration 3600: 0.202980\n",
            "Cost after iteration 3700: 0.202603\n",
            "Cost after iteration 3800: 0.202841\n",
            "Cost after iteration 3900: 0.201428\n",
            "Cost after iteration 4000: 0.201210\n",
            "Cost after iteration 4100: 0.200464\n",
            "Cost after iteration 4200: 0.199717\n",
            "Cost after iteration 4300: 0.199089\n",
            "Cost after iteration 4400: 0.198509\n",
            "Cost after iteration 4500: 0.198282\n",
            "Cost after iteration 4600: 0.197665\n",
            "Cost after iteration 4700: 0.197037\n",
            "Cost after iteration 4800: 0.196452\n",
            "Cost after iteration 4900: 0.195933\n",
            "Cost after iteration 5000: 0.195379\n",
            "Cost after iteration 5100: 0.194859\n",
            "Cost after iteration 5200: 0.194331\n",
            "Cost after iteration 5300: 0.193898\n",
            "Cost after iteration 5400: 0.193333\n",
            "Cost after iteration 5500: 0.192874\n",
            "Cost after iteration 5600: 0.192469\n",
            "Cost after iteration 5700: 0.191882\n",
            "Cost after iteration 5800: 0.191433\n",
            "Cost after iteration 5900: 0.191011\n",
            "Cost after iteration 6000: 0.190548\n",
            "Cost after iteration 6100: 0.190148\n",
            "Cost after iteration 6200: 0.189700\n",
            "Cost after iteration 6300: 0.189200\n",
            "Cost after iteration 6400: 0.188866\n",
            "Cost after iteration 6500: 0.188479\n",
            "Cost after iteration 6600: 0.188058\n",
            "Cost after iteration 6700: 0.187688\n",
            "Cost after iteration 6800: 0.187245\n",
            "Cost after iteration 6900: 0.186835\n",
            "Cost after iteration 7000: 0.186510\n",
            "Cost after iteration 7100: 0.186078\n",
            "Cost after iteration 7200: 0.185819\n",
            "Cost after iteration 7300: 0.185477\n",
            "Cost after iteration 7400: 0.185121\n",
            "Cost after iteration 7500: 0.184992\n",
            "Cost after iteration 7600: 0.184553\n",
            "Cost after iteration 7700: 0.184216\n",
            "Cost after iteration 7800: 0.183865\n",
            "Cost after iteration 7900: 0.183638\n",
            "Cost after iteration 8000: 0.183389\n",
            "Cost after iteration 8100: 0.183023\n",
            "Cost after iteration 8200: 0.182945\n",
            "Cost after iteration 8300: 0.182517\n",
            "Cost after iteration 8400: 0.182177\n",
            "Cost after iteration 8500: 0.181959\n",
            "Cost after iteration 8600: 0.180667\n",
            "Cost after iteration 8700: 0.181205\n",
            "Cost after iteration 8800: 0.180990\n",
            "Cost after iteration 8900: 0.180761\n",
            "Cost after iteration 9000: 0.180473\n",
            "Cost after iteration 9100: 0.180178\n",
            "Cost after iteration 9200: 0.179948\n",
            "Cost after iteration 9300: 0.179682\n",
            "Cost after iteration 9400: 0.179427\n",
            "Cost after iteration 9500: 0.179289\n",
            "Cost after iteration 9600: 0.178989\n",
            "Cost after iteration 9700: 0.178748\n",
            "Cost after iteration 9800: 0.178610\n",
            "Cost after iteration 9900: 0.178228\n",
            "Cost after iteration 10000: 0.178072\n",
            "Cost after iteration 10100: 0.177853\n",
            "Cost after iteration 10200: 0.177658\n",
            "Cost after iteration 10300: 0.177527\n",
            "Cost after iteration 10400: 0.177596\n",
            "Cost after iteration 10500: 0.177318\n",
            "Cost after iteration 10600: 0.177195\n",
            "Cost after iteration 10700: 0.176928\n",
            "Cost after iteration 10800: 0.176808\n",
            "Cost after iteration 10900: 0.175947\n",
            "Cost after iteration 11000: 0.176303\n",
            "Cost after iteration 11100: 0.175904\n",
            "Cost after iteration 11200: 0.175826\n",
            "Cost after iteration 11300: 0.176036\n",
            "Cost after iteration 11400: 0.176239\n",
            "Cost after iteration 11500: 0.175732\n",
            "Cost after iteration 11600: 0.176503\n",
            "Cost after iteration 11700: 0.175010\n",
            "Cost after iteration 11800: 0.174709\n",
            "Cost after iteration 11900: 0.174669\n",
            "Cost after iteration 12000: 0.175171\n",
            "Cost after iteration 12100: 0.175229\n",
            "Cost after iteration 12200: 0.174723\n",
            "Cost after iteration 12300: 0.174607\n",
            "Cost after iteration 12400: 0.174581\n",
            "Cost after iteration 12500: 0.174014\n",
            "Cost after iteration 12600: 0.174027\n",
            "Cost after iteration 12700: 0.174031\n",
            "Cost after iteration 12800: 0.173975\n",
            "Cost after iteration 12900: 0.174295\n",
            "Cost after iteration 13000: 0.174271\n",
            "Cost after iteration 13100: 0.174187\n",
            "Cost after iteration 13200: 0.174163\n",
            "Cost after iteration 13300: 0.173151\n",
            "Cost after iteration 13400: 0.172866\n",
            "Cost after iteration 13500: 0.173447\n",
            "Cost after iteration 13600: 0.173668\n",
            "Cost after iteration 13700: 0.172467\n",
            "Cost after iteration 13800: 0.173109\n",
            "Cost after iteration 13900: 0.173746\n",
            "Cost after iteration 14000: 0.173218\n",
            "Cost after iteration 14100: 0.173326\n",
            "Cost after iteration 14200: 0.172881\n",
            "Cost after iteration 14300: 0.172615\n",
            "Cost after iteration 14400: 0.172687\n",
            "Cost after iteration 14500: 0.172048\n",
            "Cost after iteration 14600: 0.172084\n",
            "Cost after iteration 14700: 0.171979\n",
            "Cost after iteration 14800: 0.171847\n",
            "Cost after iteration 14900: 0.171714\n",
            "Cost after iteration 15000: 0.171836\n",
            "Cost after iteration 15100: 0.171238\n",
            "Cost after iteration 15200: 0.172244\n",
            "Cost after iteration 15300: 0.172252\n",
            "Cost after iteration 15400: 0.169832\n",
            "Cost after iteration 15500: 0.171953\n",
            "Cost after iteration 15600: 0.172133\n",
            "Cost after iteration 15700: 0.171869\n",
            "Cost after iteration 15800: 0.170617\n",
            "Cost after iteration 15900: 0.170641\n",
            "Cost after iteration 16000: 0.170326\n",
            "Cost after iteration 16100: 0.171436\n",
            "Cost after iteration 16200: 0.170205\n",
            "Cost after iteration 16300: 0.170280\n",
            "Cost after iteration 16400: 0.170402\n",
            "Cost after iteration 16500: 0.170507\n",
            "Cost after iteration 16600: 0.170227\n",
            "Cost after iteration 16700: 0.169892\n",
            "Cost after iteration 16800: 0.169995\n",
            "Cost after iteration 16900: 0.169976\n",
            "Cost after iteration 17000: 0.169641\n",
            "Cost after iteration 17100: 0.169896\n",
            "Cost after iteration 17200: 0.169752\n",
            "Cost after iteration 17300: 0.169668\n",
            "Cost after iteration 17400: 0.169618\n",
            "Cost after iteration 17500: 0.169589\n",
            "Cost after iteration 17600: 0.169468\n",
            "Cost after iteration 17700: 0.170029\n",
            "Cost after iteration 17800: 0.169615\n",
            "Cost after iteration 17900: 0.171076\n",
            "Cost after iteration 18000: 0.168410\n",
            "Cost after iteration 18100: 0.169349\n",
            "Cost after iteration 18200: 0.169162\n",
            "Cost after iteration 18300: 0.169017\n",
            "Cost after iteration 18400: 0.167925\n",
            "Cost after iteration 18500: 0.169064\n",
            "Cost after iteration 18600: 0.168629\n",
            "Cost after iteration 18700: 0.168587\n",
            "Cost after iteration 18800: 0.168741\n",
            "Cost after iteration 18900: 0.168538\n",
            "Cost after iteration 19000: 0.168445\n",
            "Cost after iteration 19100: 0.168333\n",
            "Cost after iteration 19200: 0.168240\n",
            "Cost after iteration 19300: 0.168230\n",
            "Cost after iteration 19400: 0.168140\n",
            "Cost after iteration 19500: 0.168108\n",
            "Cost after iteration 19600: 0.167980\n",
            "Cost after iteration 19700: 0.167860\n",
            "Cost after iteration 19800: 0.167755\n",
            "Cost after iteration 19900: 0.167494\n",
            "Cost after iteration 20000: 0.167497\n",
            "Cost after iteration 20100: 0.167994\n",
            "Cost after iteration 20200: 0.167307\n",
            "Cost after iteration 20300: 0.167353\n",
            "Cost after iteration 20400: 0.166946\n",
            "Cost after iteration 20500: 0.167705\n",
            "Cost after iteration 20600: 0.167293\n",
            "Cost after iteration 20700: 0.166891\n",
            "Cost after iteration 20800: 0.167072\n",
            "Cost after iteration 20900: 0.166267\n",
            "Cost after iteration 21000: 0.167409\n",
            "Cost after iteration 21100: 0.167503\n",
            "Cost after iteration 21200: 0.165151\n",
            "Cost after iteration 21300: 0.166813\n",
            "Cost after iteration 21400: 0.166548\n",
            "Cost after iteration 21500: 0.166665\n",
            "Cost after iteration 21600: 0.166425\n",
            "Cost after iteration 21700: 0.166284\n",
            "Cost after iteration 21800: 0.166347\n",
            "Cost after iteration 21900: 0.165475\n",
            "Cost after iteration 22000: 0.166106\n",
            "Cost after iteration 22100: 0.166172\n",
            "Cost after iteration 22200: 0.165909\n",
            "Cost after iteration 22300: 0.165328\n",
            "Cost after iteration 22400: 0.165253\n",
            "Cost after iteration 22500: 0.165467\n",
            "Cost after iteration 22600: 0.165452\n",
            "Cost after iteration 22700: 0.165277\n",
            "Cost after iteration 22800: 0.165302\n",
            "Cost after iteration 22900: 0.164956\n",
            "Cost after iteration 23000: 0.164863\n",
            "Cost after iteration 23100: 0.164387\n",
            "Cost after iteration 23200: 0.165251\n",
            "Cost after iteration 23300: 0.164604\n",
            "Cost after iteration 23400: 0.164056\n",
            "Cost after iteration 23500: 0.165992\n",
            "Cost after iteration 23600: 0.163819\n",
            "Cost after iteration 23700: 0.164681\n",
            "Cost after iteration 23800: 0.161849\n",
            "Cost after iteration 23900: 0.163528\n",
            "Cost after iteration 24000: 0.165031\n",
            "Cost after iteration 24100: 0.162042\n",
            "Cost after iteration 24200: 0.165920\n",
            "Cost after iteration 24300: 0.162423\n",
            "Cost after iteration 24400: 0.164016\n",
            "Cost after iteration 24500: 0.162453\n",
            "Cost after iteration 24600: 0.161786\n",
            "Cost after iteration 24700: 0.162419\n",
            "Cost after iteration 24800: 0.161544\n",
            "Cost after iteration 24900: 0.161495\n",
            "Cost after iteration 25000: 0.162226\n",
            "Cost after iteration 25100: 0.164709\n",
            "Cost after iteration 25200: 0.162712\n",
            "Cost after iteration 25300: 0.169982\n",
            "Cost after iteration 25400: 0.161114\n",
            "Cost after iteration 25500: 0.161840\n",
            "Cost after iteration 25600: 0.158438\n",
            "Cost after iteration 25700: 0.161586\n",
            "Cost after iteration 25800: 0.160011\n",
            "Cost after iteration 25900: 0.159153\n",
            "Cost after iteration 26000: 0.161601\n",
            "Cost after iteration 26100: 0.159400\n",
            "Cost after iteration 26200: 0.162744\n",
            "Cost after iteration 26300: 0.161681\n",
            "Cost after iteration 26400: 0.161684\n",
            "Cost after iteration 26500: 0.162584\n",
            "Cost after iteration 26600: 0.162436\n",
            "Cost after iteration 26700: 0.162321\n",
            "Cost after iteration 26800: 0.162237\n",
            "Cost after iteration 26900: 0.162097\n",
            "Cost after iteration 27000: 0.161817\n",
            "Cost after iteration 27100: 0.162169\n",
            "Cost after iteration 27200: 0.162008\n",
            "Cost after iteration 27300: 0.161327\n",
            "Cost after iteration 27400: 0.161729\n",
            "Cost after iteration 27500: 0.161460\n",
            "Cost after iteration 27600: 0.161487\n",
            "Cost after iteration 27700: 0.161169\n",
            "Cost after iteration 27800: 0.161751\n",
            "Cost after iteration 27900: 0.161500\n",
            "Cost after iteration 28000: 0.161590\n",
            "Cost after iteration 28100: 0.161265\n",
            "Cost after iteration 28200: 0.160536\n",
            "Cost after iteration 28300: 0.160672\n",
            "Cost after iteration 28400: 0.159900\n",
            "Cost after iteration 28500: 0.160678\n",
            "Cost after iteration 28600: 0.160551\n",
            "Cost after iteration 28700: 0.159968\n",
            "Cost after iteration 28800: 0.160368\n",
            "Cost after iteration 28900: 0.159433\n",
            "Cost after iteration 29000: 0.159644\n",
            "Cost after iteration 29100: 0.160633\n",
            "Cost after iteration 29200: 0.160208\n",
            "Cost after iteration 29300: 0.159997\n",
            "Cost after iteration 29400: 0.160525\n",
            "Cost after iteration 29500: 0.159911\n",
            "Cost after iteration 29600: 0.160306\n",
            "Cost after iteration 29700: 0.159989\n",
            "Cost after iteration 29800: 0.159908\n",
            "Cost after iteration 29900: 0.154421\n",
            "Cost after iteration 30000: 0.162537\n",
            "Cost after iteration 30100: 0.161967\n",
            "Cost after iteration 30200: 0.157518\n",
            "Cost after iteration 30300: 0.158158\n",
            "Cost after iteration 30400: 0.161359\n",
            "Cost after iteration 30500: 0.161013\n",
            "Cost after iteration 30600: 0.157144\n",
            "Cost after iteration 30700: 0.155593\n",
            "Cost after iteration 30800: 0.154177\n",
            "Cost after iteration 30900: 0.154661\n",
            "Cost after iteration 31000: 0.165098\n",
            "Cost after iteration 31100: 0.163379\n",
            "Cost after iteration 31200: 0.154365\n",
            "Cost after iteration 31300: 0.163093\n",
            "Cost after iteration 31400: 0.162920\n",
            "Cost after iteration 31500: 0.154332\n",
            "Cost after iteration 31600: 0.156080\n",
            "Cost after iteration 31700: 0.162028\n",
            "Cost after iteration 31800: 0.160231\n",
            "Cost after iteration 31900: 0.156068\n",
            "Cost after iteration 32000: 0.154835\n",
            "Cost after iteration 32100: 0.157165\n",
            "Cost after iteration 32200: 0.157249\n",
            "Cost after iteration 32300: 0.155559\n",
            "Cost after iteration 32400: 0.157629\n",
            "Cost after iteration 32500: 0.153400\n",
            "Cost after iteration 32600: 0.157407\n",
            "Cost after iteration 32700: 0.157251\n",
            "Cost after iteration 32800: 0.157343\n",
            "Cost after iteration 32900: 0.157197\n",
            "Cost after iteration 33000: 0.156675\n",
            "Cost after iteration 33100: 0.155121\n",
            "Cost after iteration 33200: 0.156053\n",
            "Cost after iteration 33300: 0.163613\n",
            "Cost after iteration 33400: 0.157843\n",
            "Cost after iteration 33500: 0.156414\n",
            "Cost after iteration 33600: 0.154375\n",
            "Cost after iteration 33700: 0.162090\n",
            "Cost after iteration 33800: 0.155579\n",
            "Cost after iteration 33900: 0.156015\n",
            "Cost after iteration 34000: 0.156140\n",
            "Cost after iteration 34100: 0.159614\n",
            "Cost after iteration 34200: 0.158573\n",
            "Cost after iteration 34300: 0.154652\n",
            "Cost after iteration 34400: 0.154809\n",
            "Cost after iteration 34500: 0.154982\n",
            "Cost after iteration 34600: 0.160694\n",
            "Cost after iteration 34700: 0.152148\n",
            "Cost after iteration 34800: 0.166323\n",
            "Cost after iteration 34900: 0.151752\n",
            "Cost after iteration 35000: 0.160879\n",
            "Cost after iteration 35100: 0.161389\n",
            "Cost after iteration 35200: 0.155468\n",
            "Cost after iteration 35300: 0.148527\n",
            "Cost after iteration 35400: 0.163056\n",
            "Cost after iteration 35500: 0.155301\n",
            "Cost after iteration 35600: 0.158311\n",
            "Cost after iteration 35700: 0.154880\n",
            "Cost after iteration 35800: 0.151971\n",
            "Cost after iteration 35900: 0.154200\n",
            "Cost after iteration 36000: 0.156772\n",
            "Cost after iteration 36100: 0.157062\n",
            "Cost after iteration 36200: 0.147172\n",
            "Cost after iteration 36300: 0.162592\n",
            "Cost after iteration 36400: 0.165168\n",
            "Cost after iteration 36500: 0.149979\n",
            "Cost after iteration 36600: 0.156383\n",
            "Cost after iteration 36700: 0.145842\n",
            "Cost after iteration 36800: 0.156996\n",
            "Cost after iteration 36900: 0.159074\n",
            "Cost after iteration 37000: 0.153871\n",
            "Cost after iteration 37100: 0.156468\n",
            "Cost after iteration 37200: 0.153625\n",
            "Cost after iteration 37300: 0.155329\n",
            "Cost after iteration 37400: 0.153367\n",
            "Cost after iteration 37500: 0.157008\n",
            "Cost after iteration 37600: 0.162647\n",
            "Cost after iteration 37700: 0.145500\n",
            "Cost after iteration 37800: 0.153578\n",
            "Cost after iteration 37900: 0.152577\n",
            "Cost after iteration 38000: 0.153671\n",
            "Cost after iteration 38100: 0.153290\n",
            "Cost after iteration 38200: 0.151468\n",
            "Cost after iteration 38300: 0.153930\n",
            "Cost after iteration 38400: 0.154365\n",
            "Cost after iteration 38500: 0.153651\n",
            "Cost after iteration 38600: 0.152648\n",
            "Cost after iteration 38700: 0.148805\n",
            "Cost after iteration 38800: 0.152899\n",
            "Cost after iteration 38900: 0.149387\n",
            "Cost after iteration 39000: 0.151740\n",
            "Cost after iteration 39100: 0.151394\n",
            "Cost after iteration 39200: 0.156073\n",
            "Cost after iteration 39300: 0.151908\n",
            "Cost after iteration 39400: 0.155576\n",
            "Cost after iteration 39500: 0.158121\n",
            "Cost after iteration 39600: 0.148011\n",
            "Cost after iteration 39700: 0.148675\n",
            "Cost after iteration 39800: 0.157009\n",
            "Cost after iteration 39900: 0.150819\n",
            "Cost after iteration 40000: 0.147563\n",
            "Cost after iteration 40100: 0.159755\n",
            "Cost after iteration 40200: 0.149293\n",
            "Cost after iteration 40300: 0.150805\n",
            "Cost after iteration 40400: 0.154316\n",
            "Cost after iteration 40500: 0.154108\n",
            "Cost after iteration 40600: 0.149149\n",
            "Cost after iteration 40700: 0.153292\n",
            "Cost after iteration 40800: 0.144810\n",
            "Cost after iteration 40900: 0.155627\n",
            "Cost after iteration 41000: 0.156899\n",
            "Cost after iteration 41100: 0.148884\n",
            "Cost after iteration 41200: 0.151098\n",
            "Cost after iteration 41300: 0.163033\n",
            "Cost after iteration 41400: 0.148367\n",
            "Cost after iteration 41500: 0.148823\n",
            "Cost after iteration 41600: 0.152863\n",
            "Cost after iteration 41700: 0.145971\n",
            "Cost after iteration 41800: 0.155757\n",
            "Cost after iteration 41900: 0.147816\n",
            "Cost after iteration 42000: 0.153006\n",
            "Cost after iteration 42100: 0.150809\n",
            "Cost after iteration 42200: 0.143248\n",
            "Cost after iteration 42300: 0.151922\n",
            "Cost after iteration 42400: 0.155314\n",
            "Cost after iteration 42500: 0.146507\n",
            "Cost after iteration 42600: 0.144520\n",
            "Cost after iteration 42700: 0.143774\n",
            "Cost after iteration 42800: 0.168006\n",
            "Cost after iteration 42900: 0.146788\n",
            "Cost after iteration 43000: 0.150073\n",
            "Cost after iteration 43100: 0.147214\n",
            "Cost after iteration 43200: 0.145310\n",
            "Cost after iteration 43300: 0.148884\n",
            "Cost after iteration 43400: 0.153319\n",
            "Cost after iteration 43500: 0.150123\n",
            "Cost after iteration 43600: 0.150161\n",
            "Cost after iteration 43700: 0.140541\n",
            "Cost after iteration 43800: 0.143055\n",
            "Cost after iteration 43900: 0.148243\n",
            "Cost after iteration 44000: 0.152719\n",
            "Cost after iteration 44100: 0.140811\n",
            "Cost after iteration 44200: 0.148669\n",
            "Cost after iteration 44300: 0.148580\n",
            "Cost after iteration 44400: 0.145062\n",
            "Cost after iteration 44500: 0.141255\n",
            "Cost after iteration 44600: 0.155773\n",
            "Cost after iteration 44700: 0.152850\n",
            "Cost after iteration 44800: 0.145501\n",
            "Cost after iteration 44900: 0.141908\n",
            "Cost after iteration 45000: 0.146760\n",
            "Cost after iteration 45100: 0.149817\n",
            "Cost after iteration 45200: 0.136623\n",
            "Cost after iteration 45300: 0.142549\n",
            "Cost after iteration 45400: 0.148419\n",
            "Cost after iteration 45500: 0.138766\n",
            "Cost after iteration 45600: 0.145411\n",
            "Cost after iteration 45700: 0.141401\n",
            "Cost after iteration 45800: 0.143066\n",
            "Cost after iteration 45900: 0.154759\n",
            "Cost after iteration 46000: 0.142769\n",
            "Cost after iteration 46100: 0.134255\n",
            "Cost after iteration 46200: 0.152824\n",
            "Cost after iteration 46300: 0.144606\n",
            "Cost after iteration 46400: 0.145304\n",
            "Cost after iteration 46500: 0.141804\n",
            "Cost after iteration 46600: 0.146424\n",
            "Cost after iteration 46700: 0.149401\n",
            "Cost after iteration 46800: 0.148564\n",
            "Cost after iteration 46900: 0.149963\n",
            "Cost after iteration 47000: 0.139052\n",
            "Cost after iteration 47100: 0.147613\n",
            "Cost after iteration 47200: 0.140039\n",
            "Cost after iteration 47300: 0.144035\n",
            "Cost after iteration 47400: 0.140131\n",
            "Cost after iteration 47500: 0.143378\n",
            "Cost after iteration 47600: 0.153378\n",
            "Cost after iteration 47700: 0.138226\n",
            "Cost after iteration 47800: 0.141293\n",
            "Cost after iteration 47900: 0.148668\n",
            "Cost after iteration 48000: 0.150042\n",
            "Cost after iteration 48100: 0.138197\n",
            "Cost after iteration 48200: 0.144366\n",
            "Cost after iteration 48300: 0.144797\n",
            "Cost after iteration 48400: 0.143684\n",
            "Cost after iteration 48500: 0.149297\n",
            "Cost after iteration 48600: 0.136768\n",
            "Cost after iteration 48700: 0.147007\n",
            "Cost after iteration 48800: 0.142661\n",
            "Cost after iteration 48900: 0.146166\n",
            "Cost after iteration 49000: 0.141933\n",
            "Cost after iteration 49100: 0.145409\n",
            "Cost after iteration 49200: 0.136869\n",
            "Cost after iteration 49300: 0.142531\n",
            "Cost after iteration 49400: 0.145050\n",
            "Cost after iteration 49500: 0.145429\n",
            "Cost after iteration 49600: 0.146853\n",
            "Cost after iteration 49700: 0.148443\n",
            "Cost after iteration 49800: 0.134418\n",
            "Cost after iteration 49900: 0.143384\n",
            "Cost after iteration 50000: 0.152243\n",
            "Cost after iteration 50100: 0.145881\n",
            "Cost after iteration 50200: 0.152735\n",
            "Cost after iteration 50300: 0.155880\n",
            "Cost after iteration 50400: 0.138593\n",
            "Cost after iteration 50500: 0.141204\n",
            "Cost after iteration 50600: 0.137292\n",
            "Cost after iteration 50700: 0.142569\n",
            "Cost after iteration 50800: 0.142243\n",
            "Cost after iteration 50900: 0.137900\n",
            "Cost after iteration 51000: 0.143187\n",
            "Cost after iteration 51100: 0.141325\n",
            "Cost after iteration 51200: 0.145075\n",
            "Cost after iteration 51300: 0.148797\n",
            "Cost after iteration 51400: 0.143240\n",
            "Cost after iteration 51500: 0.138357\n",
            "Cost after iteration 51600: 0.146972\n",
            "Cost after iteration 51700: 0.149679\n",
            "Cost after iteration 51800: 0.135536\n",
            "Cost after iteration 51900: 0.144830\n",
            "Cost after iteration 52000: 0.140997\n",
            "Cost after iteration 52100: 0.134775\n",
            "Cost after iteration 52200: 0.135663\n",
            "Cost after iteration 52300: 0.142912\n",
            "Cost after iteration 52400: 0.134686\n",
            "Cost after iteration 52500: 0.141303\n",
            "Cost after iteration 52600: 0.136119\n",
            "Cost after iteration 52700: 0.147477\n",
            "Cost after iteration 52800: 0.146716\n",
            "Cost after iteration 52900: 0.132615\n",
            "Cost after iteration 53000: 0.149985\n",
            "Cost after iteration 53100: 0.145519\n",
            "Cost after iteration 53200: 0.140845\n",
            "Cost after iteration 53300: 0.144561\n",
            "Cost after iteration 53400: 0.144557\n",
            "Cost after iteration 53500: 0.144527\n",
            "Cost after iteration 53600: 0.144553\n",
            "Cost after iteration 53700: 0.144514\n",
            "Cost after iteration 53800: 0.144607\n",
            "Cost after iteration 53900: 0.145108\n",
            "Cost after iteration 54000: 0.144984\n",
            "Cost after iteration 54100: 0.144269\n",
            "Cost after iteration 54200: 0.144878\n",
            "Cost after iteration 54300: 0.138725\n",
            "Cost after iteration 54400: 0.145724\n",
            "Cost after iteration 54500: 0.146131\n",
            "Cost after iteration 54600: 0.140800\n",
            "Cost after iteration 54700: 0.138600\n",
            "Cost after iteration 54800: 0.143959\n",
            "Cost after iteration 54900: 0.144295\n",
            "Cost after iteration 55000: 0.144970\n",
            "Cost after iteration 55100: 0.144391\n",
            "Cost after iteration 55200: 0.142715\n",
            "Cost after iteration 55300: 0.145618\n",
            "Cost after iteration 55400: 0.146113\n",
            "Cost after iteration 55500: 0.143493\n",
            "Cost after iteration 55600: 0.141175\n",
            "Cost after iteration 55700: 0.141144\n",
            "Cost after iteration 55800: 0.146007\n",
            "Cost after iteration 55900: 0.137520\n",
            "Cost after iteration 56000: 0.138270\n",
            "Cost after iteration 56100: 0.143262\n",
            "Cost after iteration 56200: 0.136426\n",
            "Cost after iteration 56300: 0.143107\n",
            "Cost after iteration 56400: 0.144593\n",
            "Cost after iteration 56500: 0.145760\n",
            "Cost after iteration 56600: 0.144255\n",
            "Cost after iteration 56700: 0.141589\n",
            "Cost after iteration 56800: 0.135191\n",
            "Cost after iteration 56900: 0.139421\n",
            "Cost after iteration 57000: 0.143401\n",
            "Cost after iteration 57100: 0.140901\n",
            "Cost after iteration 57200: 0.139037\n",
            "Cost after iteration 57300: 0.141667\n",
            "Cost after iteration 57400: 0.134926\n",
            "Cost after iteration 57500: 0.143478\n",
            "Cost after iteration 57600: 0.145595\n",
            "Cost after iteration 57700: 0.143694\n",
            "Cost after iteration 57800: 0.137104\n",
            "Cost after iteration 57900: 0.136619\n",
            "Cost after iteration 58000: 0.137608\n",
            "Cost after iteration 58100: 0.139101\n",
            "Cost after iteration 58200: 0.149401\n",
            "Cost after iteration 58300: 0.135732\n",
            "Cost after iteration 58400: 0.137997\n",
            "Cost after iteration 58500: 0.142265\n",
            "Cost after iteration 58600: 0.144586\n",
            "Cost after iteration 58700: 0.138908\n",
            "Cost after iteration 58800: 0.139364\n",
            "Cost after iteration 58900: 0.140019\n",
            "Cost after iteration 59000: 0.137287\n",
            "Cost after iteration 59100: 0.140242\n",
            "Cost after iteration 59200: 0.141291\n",
            "Cost after iteration 59300: 0.144375\n",
            "Cost after iteration 59400: 0.141215\n",
            "Cost after iteration 59500: 0.143000\n",
            "Cost after iteration 59600: 0.132122\n",
            "Cost after iteration 59700: 0.143092\n",
            "Cost after iteration 59800: 0.141645\n",
            "Cost after iteration 59900: 0.137639\n",
            "Cost after iteration 60000: 0.142531\n",
            "Cost after iteration 60100: 0.134799\n",
            "Cost after iteration 60200: 0.137291\n",
            "Cost after iteration 60300: 0.142731\n",
            "Cost after iteration 60400: 0.141481\n",
            "Cost after iteration 60500: 0.145945\n",
            "Cost after iteration 60600: 0.142223\n",
            "Cost after iteration 60700: 0.144943\n",
            "Cost after iteration 60800: 0.141893\n",
            "Cost after iteration 60900: 0.143430\n",
            "Cost after iteration 61000: 0.142460\n",
            "Cost after iteration 61100: 0.140351\n",
            "Cost after iteration 61200: 0.142064\n",
            "Cost after iteration 61300: 0.135375\n",
            "Cost after iteration 61400: 0.140448\n",
            "Cost after iteration 61500: 0.141206\n",
            "Cost after iteration 61600: 0.137391\n",
            "Cost after iteration 61700: 0.137506\n",
            "Cost after iteration 61800: 0.137954\n",
            "Cost after iteration 61900: 0.135970\n",
            "Cost after iteration 62000: 0.136230\n",
            "Cost after iteration 62100: 0.141803\n",
            "Cost after iteration 62200: 0.136316\n",
            "Cost after iteration 62300: 0.143057\n",
            "Cost after iteration 62400: 0.138814\n",
            "Cost after iteration 62500: 0.132291\n",
            "Cost after iteration 62600: 0.137740\n",
            "Cost after iteration 62700: 0.139935\n",
            "Cost after iteration 62800: 0.143632\n",
            "Cost after iteration 62900: 0.142194\n",
            "Cost after iteration 63000: 0.142469\n",
            "Cost after iteration 63100: 0.141781\n",
            "Cost after iteration 63200: 0.146146\n",
            "Cost after iteration 63300: 0.142626\n",
            "Cost after iteration 63400: 0.139033\n",
            "Cost after iteration 63500: 0.138703\n",
            "Cost after iteration 63600: 0.136736\n",
            "Cost after iteration 63700: 0.140156\n",
            "Cost after iteration 63800: 0.138640\n",
            "Cost after iteration 63900: 0.143156\n",
            "Cost after iteration 64000: 0.138749\n",
            "Cost after iteration 64100: 0.143844\n",
            "Cost after iteration 64200: 0.143777\n",
            "Cost after iteration 64300: 0.139550\n",
            "Cost after iteration 64400: 0.145639\n",
            "Cost after iteration 64500: 0.142673\n",
            "Cost after iteration 64600: 0.136069\n",
            "Cost after iteration 64700: 0.141856\n",
            "Cost after iteration 64800: 0.143366\n",
            "Cost after iteration 64900: 0.140356\n",
            "Cost after iteration 65000: 0.143252\n",
            "Cost after iteration 65100: 0.136117\n",
            "Cost after iteration 65200: 0.144613\n",
            "Cost after iteration 65300: 0.146453\n",
            "Cost after iteration 65400: 0.137633\n",
            "Cost after iteration 65500: 0.142963\n",
            "Cost after iteration 65600: 0.139940\n",
            "Cost after iteration 65700: 0.144462\n",
            "Cost after iteration 65800: 0.142079\n",
            "Cost after iteration 65900: 0.142079\n",
            "Cost after iteration 66000: 0.136136\n",
            "Cost after iteration 66100: 0.141775\n",
            "Cost after iteration 66200: 0.135048\n",
            "Cost after iteration 66300: 0.141196\n",
            "Cost after iteration 66400: 0.139530\n",
            "Cost after iteration 66500: 0.139390\n",
            "Cost after iteration 66600: 0.137886\n",
            "Cost after iteration 66700: 0.137046\n",
            "Cost after iteration 66800: 0.138876\n",
            "Cost after iteration 66900: 0.134466\n",
            "Cost after iteration 67000: 0.139828\n",
            "Cost after iteration 67100: 0.140276\n",
            "Cost after iteration 67200: 0.142291\n",
            "Cost after iteration 67300: 0.138967\n",
            "Cost after iteration 67400: 0.139139\n",
            "Cost after iteration 67500: 0.147437\n",
            "Cost after iteration 67600: 0.137663\n",
            "Cost after iteration 67700: 0.141285\n",
            "Cost after iteration 67800: 0.135908\n",
            "Cost after iteration 67900: 0.141468\n",
            "Cost after iteration 68000: 0.134278\n",
            "Cost after iteration 68100: 0.138163\n",
            "Cost after iteration 68200: 0.135495\n",
            "Cost after iteration 68300: 0.142639\n",
            "Cost after iteration 68400: 0.142225\n",
            "Cost after iteration 68500: 0.141821\n",
            "Cost after iteration 68600: 0.139911\n",
            "Cost after iteration 68700: 0.145965\n",
            "Cost after iteration 68800: 0.135728\n",
            "Cost after iteration 68900: 0.146150\n",
            "Cost after iteration 69000: 0.134514\n",
            "Cost after iteration 69100: 0.135462\n",
            "Cost after iteration 69200: 0.139692\n",
            "Cost after iteration 69300: 0.141162\n",
            "Cost after iteration 69400: 0.149546\n",
            "Cost after iteration 69500: 0.138396\n",
            "Cost after iteration 69600: 0.144448\n",
            "Cost after iteration 69700: 0.132996\n",
            "Cost after iteration 69800: 0.140334\n",
            "Cost after iteration 69900: 0.137890\n",
            "Cost after iteration 70000: 0.137039\n",
            "Cost after iteration 70100: 0.138451\n",
            "Cost after iteration 70200: 0.141108\n",
            "Cost after iteration 70300: 0.140385\n",
            "Cost after iteration 70400: 0.131859\n",
            "Cost after iteration 70500: 0.134230\n",
            "Cost after iteration 70600: 0.146557\n",
            "Cost after iteration 70700: 0.131609\n",
            "Cost after iteration 70800: 0.138140\n",
            "Cost after iteration 70900: 0.135094\n",
            "Cost after iteration 71000: 0.131808\n",
            "Cost after iteration 71100: 0.136442\n",
            "Cost after iteration 71200: 0.137925\n",
            "Cost after iteration 71300: 0.140187\n",
            "Cost after iteration 71400: 0.138458\n",
            "Cost after iteration 71500: 0.146224\n",
            "Cost after iteration 71600: 0.143127\n",
            "Cost after iteration 71700: 0.142045\n",
            "Cost after iteration 71800: 0.142172\n",
            "Cost after iteration 71900: 0.137557\n",
            "Cost after iteration 72000: 0.135372\n",
            "Cost after iteration 72100: 0.148251\n",
            "Cost after iteration 72200: 0.136247\n",
            "Cost after iteration 72300: 0.137075\n",
            "Cost after iteration 72400: 0.142609\n",
            "Cost after iteration 72500: 0.137433\n",
            "Cost after iteration 72600: 0.143609\n",
            "Cost after iteration 72700: 0.133896\n",
            "Cost after iteration 72800: 0.146318\n",
            "Cost after iteration 72900: 0.137091\n",
            "Cost after iteration 73000: 0.139973\n",
            "Cost after iteration 73100: 0.142892\n",
            "Cost after iteration 73200: 0.137341\n",
            "Cost after iteration 73300: 0.146048\n",
            "Cost after iteration 73400: 0.139444\n",
            "Cost after iteration 73500: 0.134474\n",
            "Cost after iteration 73600: 0.143853\n",
            "Cost after iteration 73700: 0.136756\n",
            "Cost after iteration 73800: 0.138164\n",
            "Cost after iteration 73900: 0.131065\n",
            "Cost after iteration 74000: 0.132896\n",
            "Cost after iteration 74100: 0.137805\n",
            "Cost after iteration 74200: 0.145744\n",
            "Cost after iteration 74300: 0.138267\n",
            "Cost after iteration 74400: 0.130800\n",
            "Cost after iteration 74500: 0.137754\n",
            "Cost after iteration 74600: 0.136222\n",
            "Cost after iteration 74700: 0.135075\n",
            "Cost after iteration 74800: 0.133266\n",
            "Cost after iteration 74900: 0.142349\n",
            "Cost after iteration 75000: 0.140676\n",
            "Cost after iteration 75100: 0.138905\n",
            "Cost after iteration 75200: 0.140007\n",
            "Cost after iteration 75300: 0.139745\n",
            "Cost after iteration 75400: 0.139013\n",
            "Cost after iteration 75500: 0.130257\n",
            "Cost after iteration 75600: 0.145049\n",
            "Cost after iteration 75700: 0.135563\n",
            "Cost after iteration 75800: 0.142958\n",
            "Cost after iteration 75900: 0.144513\n",
            "Cost after iteration 76000: 0.130205\n",
            "Cost after iteration 76100: 0.134074\n",
            "Cost after iteration 76200: 0.148580\n",
            "Cost after iteration 76300: 0.134279\n",
            "Cost after iteration 76400: 0.141573\n",
            "Cost after iteration 76500: 0.130369\n",
            "Cost after iteration 76600: 0.139751\n",
            "Cost after iteration 76700: 0.141310\n",
            "Cost after iteration 76800: 0.136743\n",
            "Cost after iteration 76900: 0.136881\n",
            "Cost after iteration 77000: 0.135355\n",
            "Cost after iteration 77100: 0.141108\n",
            "Cost after iteration 77200: 0.134736\n",
            "Cost after iteration 77300: 0.135445\n",
            "Cost after iteration 77400: 0.135154\n",
            "Cost after iteration 77500: 0.135454\n",
            "Cost after iteration 77600: 0.138741\n",
            "Cost after iteration 77700: 0.138625\n",
            "Cost after iteration 77800: 0.135034\n",
            "Cost after iteration 77900: 0.141003\n",
            "Cost after iteration 78000: 0.140537\n",
            "Cost after iteration 78100: 0.147224\n",
            "Cost after iteration 78200: 0.135094\n",
            "Cost after iteration 78300: 0.138661\n",
            "Cost after iteration 78400: 0.132954\n",
            "Cost after iteration 78500: 0.131561\n",
            "Cost after iteration 78600: 0.142876\n",
            "Cost after iteration 78700: 0.139007\n",
            "Cost after iteration 78800: 0.128386\n",
            "Cost after iteration 78900: 0.136792\n",
            "Cost after iteration 79000: 0.134718\n",
            "Cost after iteration 79100: 0.139393\n",
            "Cost after iteration 79200: 0.138557\n",
            "Cost after iteration 79300: 0.130337\n",
            "Cost after iteration 79400: 0.140193\n",
            "Cost after iteration 79500: 0.130472\n",
            "Cost after iteration 79600: 0.138433\n",
            "Cost after iteration 79700: 0.135064\n",
            "Cost after iteration 79800: 0.137697\n",
            "Cost after iteration 79900: 0.141381\n",
            "Cost after iteration 80000: 0.131861\n",
            "Cost after iteration 80100: 0.148355\n",
            "Cost after iteration 80200: 0.130876\n",
            "Cost after iteration 80300: 0.138264\n",
            "Cost after iteration 80400: 0.144576\n",
            "Cost after iteration 80500: 0.134811\n",
            "Cost after iteration 80600: 0.136224\n",
            "Cost after iteration 80700: 0.150536\n",
            "Cost after iteration 80800: 0.136220\n",
            "Cost after iteration 80900: 0.142065\n",
            "Cost after iteration 81000: 0.133982\n",
            "Cost after iteration 81100: 0.137393\n",
            "Cost after iteration 81200: 0.143522\n",
            "Cost after iteration 81300: 0.140624\n",
            "Cost after iteration 81400: 0.127818\n",
            "Cost after iteration 81500: 0.135176\n",
            "Cost after iteration 81600: 0.146217\n",
            "Cost after iteration 81700: 0.131462\n",
            "Cost after iteration 81800: 0.146823\n",
            "Cost after iteration 81900: 0.141983\n",
            "Cost after iteration 82000: 0.138410\n",
            "Cost after iteration 82100: 0.138298\n",
            "Cost after iteration 82200: 0.138261\n",
            "Cost after iteration 82300: 0.133017\n",
            "Cost after iteration 82400: 0.132308\n",
            "Cost after iteration 82500: 0.134083\n",
            "Cost after iteration 82600: 0.134395\n",
            "Cost after iteration 82700: 0.147655\n",
            "Cost after iteration 82800: 0.144307\n",
            "Cost after iteration 82900: 0.137949\n",
            "Cost after iteration 83000: 0.138238\n",
            "Cost after iteration 83100: 0.137770\n",
            "Cost after iteration 83200: 0.134226\n",
            "Cost after iteration 83300: 0.142301\n",
            "Cost after iteration 83400: 0.138023\n",
            "Cost after iteration 83500: 0.137617\n",
            "Cost after iteration 83600: 0.137581\n",
            "Cost after iteration 83700: 0.137505\n",
            "Cost after iteration 83800: 0.129480\n",
            "Cost after iteration 83900: 0.130148\n",
            "Cost after iteration 84000: 0.131216\n",
            "Cost after iteration 84100: 0.133698\n",
            "Cost after iteration 84200: 0.133047\n",
            "Cost after iteration 84300: 0.133015\n",
            "Cost after iteration 84400: 0.128147\n",
            "Cost after iteration 84500: 0.133954\n",
            "Cost after iteration 84600: 0.142942\n",
            "Cost after iteration 84700: 0.137909\n",
            "Cost after iteration 84800: 0.137640\n",
            "Cost after iteration 84900: 0.137204\n",
            "Cost after iteration 85000: 0.137119\n",
            "Cost after iteration 85100: 0.136975\n",
            "Cost after iteration 85200: 0.129478\n",
            "Cost after iteration 85300: 0.139665\n",
            "Cost after iteration 85400: 0.146506\n",
            "Cost after iteration 85500: 0.144774\n",
            "Cost after iteration 85600: 0.137781\n",
            "Cost after iteration 85700: 0.137283\n",
            "Cost after iteration 85800: 0.137091\n",
            "Cost after iteration 85900: 0.136866\n",
            "Cost after iteration 86000: 0.136697\n",
            "Cost after iteration 86100: 0.136598\n",
            "Cost after iteration 86200: 0.136564\n",
            "Cost after iteration 86300: 0.136520\n",
            "Cost after iteration 86400: 0.129259\n",
            "Cost after iteration 86500: 0.130654\n",
            "Cost after iteration 86600: 0.141347\n",
            "Cost after iteration 86700: 0.142385\n",
            "Cost after iteration 86800: 0.139147\n",
            "Cost after iteration 86900: 0.139025\n",
            "Cost after iteration 87000: 0.140421\n",
            "Cost after iteration 87100: 0.137385\n",
            "Cost after iteration 87200: 0.136464\n",
            "Cost after iteration 87300: 0.136507\n",
            "Cost after iteration 87400: 0.136280\n",
            "Cost after iteration 87500: 0.135952\n",
            "Cost after iteration 87600: 0.135905\n",
            "Cost after iteration 87700: 0.135800\n",
            "Cost after iteration 87800: 0.135417\n",
            "Cost after iteration 87900: 0.136180\n",
            "Cost after iteration 88000: 0.136092\n",
            "Cost after iteration 88100: 0.135944\n",
            "Cost after iteration 88200: 0.136080\n",
            "Cost after iteration 88300: 0.135519\n",
            "Cost after iteration 88400: 0.135742\n",
            "Cost after iteration 88500: 0.135537\n",
            "Cost after iteration 88600: 0.136050\n",
            "Cost after iteration 88700: 0.135458\n",
            "Cost after iteration 88800: 0.135567\n",
            "Cost after iteration 88900: 0.135499\n",
            "Cost after iteration 89000: 0.136350\n",
            "Cost after iteration 89100: 0.135243\n",
            "Cost after iteration 89200: 0.135167\n",
            "Cost after iteration 89300: 0.135501\n",
            "Cost after iteration 89400: 0.135885\n",
            "Cost after iteration 89500: 0.135683\n",
            "Cost after iteration 89600: 0.136089\n",
            "Cost after iteration 89700: 0.135557\n",
            "Cost after iteration 89800: 0.135544\n",
            "Cost after iteration 89900: 0.135318\n",
            "Cost after iteration 90000: 0.135461\n",
            "Cost after iteration 90100: 0.134984\n",
            "Cost after iteration 90200: 0.135113\n",
            "Cost after iteration 90300: 0.135557\n",
            "Cost after iteration 90400: 0.135201\n",
            "Cost after iteration 90500: 0.135421\n",
            "Cost after iteration 90600: 0.135749\n",
            "Cost after iteration 90700: 0.134865\n",
            "Cost after iteration 90800: 0.135080\n",
            "Cost after iteration 90900: 0.135619\n",
            "Cost after iteration 91000: 0.134897\n",
            "Cost after iteration 91100: 0.135153\n",
            "Cost after iteration 91200: 0.134920\n",
            "Cost after iteration 91300: 0.134871\n",
            "Cost after iteration 91400: 0.134952\n",
            "Cost after iteration 91500: 0.134963\n",
            "Cost after iteration 91600: 0.135055\n",
            "Cost after iteration 91700: 0.134667\n",
            "Cost after iteration 91800: 0.134842\n",
            "Cost after iteration 91900: 0.134627\n",
            "Cost after iteration 92000: 0.134602\n",
            "Cost after iteration 92100: 0.134662\n",
            "Cost after iteration 92200: 0.134628\n",
            "Cost after iteration 92300: 0.134582\n",
            "Cost after iteration 92400: 0.134574\n",
            "Cost after iteration 92500: 0.134730\n",
            "Cost after iteration 92600: 0.134569\n",
            "Cost after iteration 92700: 0.134545\n",
            "Cost after iteration 92800: 0.134476\n",
            "Cost after iteration 92900: 0.134469\n",
            "Cost after iteration 93000: 0.134493\n",
            "Cost after iteration 93100: 0.134498\n",
            "Cost after iteration 93200: 0.134600\n",
            "Cost after iteration 93300: 0.127374\n",
            "Cost after iteration 93400: 0.137639\n",
            "Cost after iteration 93500: 0.126298\n",
            "Cost after iteration 93600: 0.134702\n",
            "Cost after iteration 93700: 0.137857\n",
            "Cost after iteration 93800: 0.126642\n",
            "Cost after iteration 93900: 0.131875\n",
            "Cost after iteration 94000: 0.132246\n",
            "Cost after iteration 94100: 0.136022\n",
            "Cost after iteration 94200: 0.134195\n",
            "Cost after iteration 94300: 0.136269\n",
            "Cost after iteration 94400: 0.136246\n",
            "Cost after iteration 94500: 0.128418\n",
            "Cost after iteration 94600: 0.139059\n",
            "Cost after iteration 94700: 0.126908\n",
            "Cost after iteration 94800: 0.139401\n",
            "Cost after iteration 94900: 0.127728\n",
            "Cost after iteration 95000: 0.133752\n",
            "Cost after iteration 95100: 0.124949\n",
            "Cost after iteration 95200: 0.136194\n",
            "Cost after iteration 95300: 0.133000\n",
            "Cost after iteration 95400: 0.135064\n",
            "Cost after iteration 95500: 0.137098\n",
            "Cost after iteration 95600: 0.139839\n",
            "Cost after iteration 95700: 0.136000\n",
            "Cost after iteration 95800: 0.135192\n",
            "Cost after iteration 95900: 0.130848\n",
            "Cost after iteration 96000: 0.125032\n",
            "Cost after iteration 96100: 0.135235\n",
            "Cost after iteration 96200: 0.131811\n",
            "Cost after iteration 96300: 0.136113\n",
            "Cost after iteration 96400: 0.135359\n",
            "Cost after iteration 96500: 0.130628\n",
            "Cost after iteration 96600: 0.134534\n",
            "Cost after iteration 96700: 0.132963\n",
            "Cost after iteration 96800: 0.134379\n",
            "Cost after iteration 96900: 0.134580\n",
            "Cost after iteration 97000: 0.137591\n",
            "Cost after iteration 97100: 0.131522\n",
            "Cost after iteration 97200: 0.137532\n",
            "Cost after iteration 97300: 0.135687\n",
            "Cost after iteration 97400: 0.134022\n",
            "Cost after iteration 97500: 0.133003\n",
            "Cost after iteration 97600: 0.134570\n",
            "Cost after iteration 97700: 0.127629\n",
            "Cost after iteration 97800: 0.133381\n",
            "Cost after iteration 97900: 0.128497\n",
            "Cost after iteration 98000: 0.135316\n",
            "Cost after iteration 98100: 0.132145\n",
            "Cost after iteration 98200: 0.136366\n",
            "Cost after iteration 98300: 0.135556\n",
            "Cost after iteration 98400: 0.137619\n",
            "Cost after iteration 98500: 0.132205\n",
            "Cost after iteration 98600: 0.136897\n",
            "Cost after iteration 98700: 0.132518\n",
            "Cost after iteration 98800: 0.136248\n",
            "Cost after iteration 98900: 0.132957\n",
            "Cost after iteration 99000: 0.134840\n",
            "Cost after iteration 99100: 0.131779\n",
            "Cost after iteration 99200: 0.134474\n",
            "Cost after iteration 99300: 0.132026\n",
            "Cost after iteration 99400: 0.132630\n",
            "Cost after iteration 99500: 0.128400\n",
            "Cost after iteration 99600: 0.130195\n",
            "Cost after iteration 99700: 0.135187\n",
            "Cost after iteration 99800: 0.130973\n",
            "Cost after iteration 99900: 0.133624\n",
            "Cost after iteration 100000: 0.132958\n",
            "Cost after iteration 100100: 0.134269\n",
            "Cost after iteration 100200: 0.133379\n",
            "Cost after iteration 100300: 0.131621\n",
            "Cost after iteration 100400: 0.127918\n",
            "Cost after iteration 100500: 0.132244\n",
            "Cost after iteration 100600: 0.133877\n",
            "Cost after iteration 100700: 0.132223\n",
            "Cost after iteration 100800: 0.131098\n",
            "Cost after iteration 100900: 0.132026\n",
            "Cost after iteration 101000: 0.132220\n",
            "Cost after iteration 101100: 0.133673\n",
            "Cost after iteration 101200: 0.133074\n",
            "Cost after iteration 101300: 0.131765\n",
            "Cost after iteration 101400: 0.130660\n",
            "Cost after iteration 101500: 0.134866\n",
            "Cost after iteration 101600: 0.133624\n",
            "Cost after iteration 101700: 0.133827\n",
            "Cost after iteration 101800: 0.132806\n",
            "Cost after iteration 101900: 0.129682\n",
            "Cost after iteration 102000: 0.133179\n",
            "Cost after iteration 102100: 0.132823\n",
            "Cost after iteration 102200: 0.131370\n",
            "Cost after iteration 102300: 0.132259\n",
            "Cost after iteration 102400: 0.131679\n",
            "Cost after iteration 102500: 0.125867\n",
            "Cost after iteration 102600: 0.131756\n",
            "Cost after iteration 102700: 0.125720\n",
            "Cost after iteration 102800: 0.131955\n",
            "Cost after iteration 102900: 0.131565\n",
            "Cost after iteration 103000: 0.129246\n",
            "Cost after iteration 103100: 0.133934\n",
            "Cost after iteration 103200: 0.130726\n",
            "Cost after iteration 103300: 0.131977\n",
            "Cost after iteration 103400: 0.132711\n",
            "Cost after iteration 103500: 0.129541\n",
            "Cost after iteration 103600: 0.131868\n",
            "Cost after iteration 103700: 0.132102\n",
            "Cost after iteration 103800: 0.134516\n",
            "Cost after iteration 103900: 0.132090\n",
            "Cost after iteration 104000: 0.131806\n",
            "Cost after iteration 104100: 0.127888\n",
            "Cost after iteration 104200: 0.132240\n",
            "Cost after iteration 104300: 0.132682\n",
            "Cost after iteration 104400: 0.131223\n",
            "Cost after iteration 104500: 0.131349\n",
            "Cost after iteration 104600: 0.133451\n",
            "Cost after iteration 104700: 0.131576\n",
            "Cost after iteration 104800: 0.132186\n",
            "Cost after iteration 104900: 0.130907\n",
            "Cost after iteration 105000: 0.131332\n",
            "Cost after iteration 105100: 0.131207\n",
            "Cost after iteration 105200: 0.130864\n",
            "Cost after iteration 105300: 0.131860\n",
            "Cost after iteration 105400: 0.132955\n",
            "Cost after iteration 105500: 0.133868\n",
            "Cost after iteration 105600: 0.129412\n",
            "Cost after iteration 105700: 0.130985\n",
            "Cost after iteration 105800: 0.131006\n",
            "Cost after iteration 105900: 0.130937\n",
            "Cost after iteration 106000: 0.130910\n",
            "Cost after iteration 106100: 0.130924\n",
            "Cost after iteration 106200: 0.131579\n",
            "Cost after iteration 106300: 0.129990\n",
            "Cost after iteration 106400: 0.130873\n",
            "Cost after iteration 106500: 0.130826\n",
            "Cost after iteration 106600: 0.130809\n",
            "Cost after iteration 106700: 0.130784\n",
            "Cost after iteration 106800: 0.130744\n",
            "Cost after iteration 106900: 0.130738\n",
            "Cost after iteration 107000: 0.130648\n",
            "Cost after iteration 107100: 0.130644\n",
            "Cost after iteration 107200: 0.130656\n",
            "Cost after iteration 107300: 0.130674\n",
            "Cost after iteration 107400: 0.130550\n",
            "Cost after iteration 107500: 0.130596\n",
            "Cost after iteration 107600: 0.130538\n",
            "Cost after iteration 107700: 0.130555\n",
            "Cost after iteration 107800: 0.130511\n",
            "Cost after iteration 107900: 0.130433\n",
            "Cost after iteration 108000: 0.130539\n",
            "Cost after iteration 108100: 0.130436\n",
            "Cost after iteration 108200: 0.130506\n",
            "Cost after iteration 108300: 0.130458\n",
            "Cost after iteration 108400: 0.130134\n",
            "Cost after iteration 108500: 0.130140\n",
            "Cost after iteration 108600: 0.130357\n",
            "Cost after iteration 108700: 0.130395\n",
            "Cost after iteration 108800: 0.130332\n",
            "Cost after iteration 108900: 0.130294\n",
            "Cost after iteration 109000: 0.129429\n",
            "Cost after iteration 109100: 0.125177\n",
            "Cost after iteration 109200: 0.131506\n",
            "Cost after iteration 109300: 0.130320\n",
            "Cost after iteration 109400: 0.128198\n",
            "Cost after iteration 109500: 0.130414\n",
            "Cost after iteration 109600: 0.130018\n",
            "Cost after iteration 109700: 0.130133\n",
            "Cost after iteration 109800: 0.129881\n",
            "Cost after iteration 109900: 0.130999\n",
            "Cost after iteration 110000: 0.130269\n",
            "Cost after iteration 110100: 0.130024\n",
            "Cost after iteration 110200: 0.130152\n",
            "Cost after iteration 110300: 0.130001\n",
            "Cost after iteration 110400: 0.129947\n",
            "Cost after iteration 110500: 0.130149\n",
            "Cost after iteration 110600: 0.130072\n",
            "Cost after iteration 110700: 0.129952\n",
            "Cost after iteration 110800: 0.129978\n",
            "Cost after iteration 110900: 0.130088\n",
            "Cost after iteration 111000: 0.129985\n",
            "Cost after iteration 111100: 0.130632\n",
            "Cost after iteration 111200: 0.130663\n",
            "Cost after iteration 111300: 0.129925\n",
            "Cost after iteration 111400: 0.129838\n",
            "Cost after iteration 111500: 0.130978\n",
            "Cost after iteration 111600: 0.126916\n",
            "Cost after iteration 111700: 0.129721\n",
            "Cost after iteration 111800: 0.129702\n",
            "Cost after iteration 111900: 0.129709\n",
            "Cost after iteration 112000: 0.129501\n",
            "Cost after iteration 112100: 0.129658\n",
            "Cost after iteration 112200: 0.129873\n",
            "Cost after iteration 112300: 0.129691\n",
            "Cost after iteration 112400: 0.129695\n",
            "Cost after iteration 112500: 0.129579\n",
            "Cost after iteration 112600: 0.130624\n",
            "Cost after iteration 112700: 0.129593\n",
            "Cost after iteration 112800: 0.129575\n",
            "Cost after iteration 112900: 0.129256\n",
            "Cost after iteration 113000: 0.129470\n",
            "Cost after iteration 113100: 0.129466\n",
            "Cost after iteration 113200: 0.129476\n",
            "Cost after iteration 113300: 0.129567\n",
            "Cost after iteration 113400: 0.129258\n",
            "Cost after iteration 113500: 0.129529\n",
            "Cost after iteration 113600: 0.129272\n",
            "Cost after iteration 113700: 0.129394\n",
            "Cost after iteration 113800: 0.129133\n",
            "Cost after iteration 113900: 0.129387\n",
            "Cost after iteration 114000: 0.129892\n",
            "Cost after iteration 114100: 0.129253\n",
            "Cost after iteration 114200: 0.129347\n",
            "Cost after iteration 114300: 0.129198\n",
            "Cost after iteration 114400: 0.129287\n",
            "Cost after iteration 114500: 0.130341\n",
            "Cost after iteration 114600: 0.129244\n",
            "Cost after iteration 114700: 0.129905\n",
            "Cost after iteration 114800: 0.129865\n",
            "Cost after iteration 114900: 0.129392\n",
            "Cost after iteration 115000: 0.128903\n",
            "Cost after iteration 115100: 0.128811\n",
            "Cost after iteration 115200: 0.129503\n",
            "Cost after iteration 115300: 0.129460\n",
            "Cost after iteration 115400: 0.129585\n",
            "Cost after iteration 115500: 0.129266\n",
            "Cost after iteration 115600: 0.129099\n",
            "Cost after iteration 115700: 0.128228\n",
            "Cost after iteration 115800: 0.128837\n",
            "Cost after iteration 115900: 0.130205\n",
            "Cost after iteration 116000: 0.128415\n",
            "Cost after iteration 116100: 0.128838\n",
            "Cost after iteration 116200: 0.128343\n",
            "Cost after iteration 116300: 0.129027\n",
            "Cost after iteration 116400: 0.129567\n",
            "Cost after iteration 116500: 0.129806\n",
            "Cost after iteration 116600: 0.130259\n",
            "Cost after iteration 116700: 0.127914\n",
            "Cost after iteration 116800: 0.129533\n",
            "Cost after iteration 116900: 0.129868\n",
            "Cost after iteration 117000: 0.129547\n",
            "Cost after iteration 117100: 0.129679\n",
            "Cost after iteration 117200: 0.128219\n",
            "Cost after iteration 117300: 0.129347\n",
            "Cost after iteration 117400: 0.128040\n",
            "Cost after iteration 117500: 0.127783\n",
            "Cost after iteration 117600: 0.128266\n",
            "Cost after iteration 117700: 0.127880\n",
            "Cost after iteration 117800: 0.128798\n",
            "Cost after iteration 117900: 0.129960\n",
            "Cost after iteration 118000: 0.129901\n",
            "Cost after iteration 118100: 0.128974\n",
            "Cost after iteration 118200: 0.129522\n",
            "Cost after iteration 118300: 0.129632\n",
            "Cost after iteration 118400: 0.127687\n",
            "Cost after iteration 118500: 0.129213\n",
            "Cost after iteration 118600: 0.129039\n",
            "Cost after iteration 118700: 0.129062\n",
            "Cost after iteration 118800: 0.129234\n",
            "Cost after iteration 118900: 0.128941\n",
            "Cost after iteration 119000: 0.128091\n",
            "Cost after iteration 119100: 0.128797\n",
            "Cost after iteration 119200: 0.128932\n",
            "Cost after iteration 119300: 0.128929\n",
            "Cost after iteration 119400: 0.128950\n",
            "Cost after iteration 119500: 0.129354\n",
            "Cost after iteration 119600: 0.128778\n",
            "Cost after iteration 119700: 0.129172\n",
            "Cost after iteration 119800: 0.129361\n",
            "Cost after iteration 119900: 0.128780\n",
            "Cost after iteration 120000: 0.129162\n",
            "Cost after iteration 120100: 0.128419\n",
            "Cost after iteration 120200: 0.129093\n",
            "Cost after iteration 120300: 0.129418\n",
            "Cost after iteration 120400: 0.128648\n",
            "Cost after iteration 120500: 0.129089\n",
            "Cost after iteration 120600: 0.128643\n",
            "Cost after iteration 120700: 0.128525\n",
            "Cost after iteration 120800: 0.128715\n",
            "Cost after iteration 120900: 0.127340\n",
            "Cost after iteration 121000: 0.129368\n",
            "Cost after iteration 121100: 0.129184\n",
            "Cost after iteration 121200: 0.129029\n",
            "Cost after iteration 121300: 0.127374\n",
            "Cost after iteration 121400: 0.129044\n",
            "Cost after iteration 121500: 0.127641\n",
            "Cost after iteration 121600: 0.128238\n",
            "Cost after iteration 121700: 0.127135\n",
            "Cost after iteration 121800: 0.127932\n",
            "Cost after iteration 121900: 0.128648\n",
            "Cost after iteration 122000: 0.128800\n",
            "Cost after iteration 122100: 0.128651\n",
            "Cost after iteration 122200: 0.128833\n",
            "Cost after iteration 122300: 0.128676\n",
            "Cost after iteration 122400: 0.128721\n",
            "Cost after iteration 122500: 0.128446\n",
            "Cost after iteration 122600: 0.128522\n",
            "Cost after iteration 122700: 0.127220\n",
            "Cost after iteration 122800: 0.129012\n",
            "Cost after iteration 122900: 0.128171\n",
            "Cost after iteration 123000: 0.127121\n",
            "Cost after iteration 123100: 0.127846\n",
            "Cost after iteration 123200: 0.128068\n",
            "Cost after iteration 123300: 0.128693\n",
            "Cost after iteration 123400: 0.128361\n",
            "Cost after iteration 123500: 0.128598\n",
            "Cost after iteration 123600: 0.128961\n",
            "Cost after iteration 123700: 0.127898\n",
            "Cost after iteration 123800: 0.129396\n",
            "Cost after iteration 123900: 0.127866\n",
            "Cost after iteration 124000: 0.127426\n",
            "Cost after iteration 124100: 0.128534\n",
            "Cost after iteration 124200: 0.127534\n",
            "Cost after iteration 124300: 0.128442\n",
            "Cost after iteration 124400: 0.128630\n",
            "Cost after iteration 124500: 0.128836\n",
            "Cost after iteration 124600: 0.128572\n",
            "Cost after iteration 124700: 0.128592\n",
            "Cost after iteration 124800: 0.131669\n",
            "Cost after iteration 124900: 0.127951\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function\n",
        "def predict(X, y, model, classes):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    model -- trained model\n",
        "    classes - number of classes, 2 for binary classification, >2 for multi-class classification\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(model.linear) # number of layers in the neural network\n",
        "\n",
        "    if classes == 2:\n",
        "      p = np.zeros((1,m))\n",
        "    else:\n",
        "      p = np.zeros((classes, m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas = model.forward(X)\n",
        "    \n",
        "    if classes == 2:\n",
        "      # convert probas to 0/1 predictions\n",
        "      for i in range(0, probas.shape[1]):\n",
        "          if probas[0,i] > 0.5:\n",
        "              p[0,i] = 1\n",
        "          else:\n",
        "              p[0,i] = 0\n",
        "\n",
        "      #print results\n",
        "      if y is not None:\n",
        "        print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "\n",
        "    else:\n",
        "      # convert probas to one hot vector predictions\n",
        "      prediction = np.argmax(probas, axis=0, out=None)\n",
        "    \n",
        "      for i in range(len(prediction)):\n",
        "          p[prediction[i], i] = 1\n",
        "\n",
        "      #print results\n",
        "      if y is not None:\n",
        "        correct = 0\n",
        "        for i in range(m):\n",
        "          if (p[:, i] == y[:, i]).all():\n",
        "            correct += 1\n",
        "        print(\"Accuracy: \"  + str(correct/m))\n",
        "        \n",
        "    return p"
      ],
      "metadata": {
        "id": "woCqucFUYXe6"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkeoJrFZznMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6f3718-c632-484f-854a-1ebe0c6aadf2"
      },
      "source": [
        "pred_train = predict(X_train, y_train, model, 2)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9474999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mERo3g41zsyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e66b77e-5723-4054-e0fd-8f22430922df"
      },
      "source": [
        "pred_val = predict(X_val, y_val, model, 2)\n",
        "output[\"basic_pred_val\"] = pred_val\n",
        "output[\"basic_layers_dims\"] = layers_dims\n",
        "output[\"basic_activation_fn\"] = activation_fn\n",
        "basic_model_parameters = []\n",
        "for basic_linear in model.linear:\n",
        "  basic_model_parameters.append(basic_linear.parameters)\n",
        "output[\"basic_model_parameters\"] = basic_model_parameters"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMCpPFMVdj36"
      },
      "source": [
        "# Advanced implementation (multi class classification)\n",
        "\n",
        "In this section, you need to implement a multi-class classifier using the functions you had previously written. You will create a model that can classify ten handwritten digits. The MNIST handwritten digit classification problem is a standard dataset in computer vision and deep learning. We usually use convolutional deep-learning neural networks for image classification. However, using only dense layers appears to be enough to handle this simple dataset, and this is a good way to get started with image datasets. \n",
        "\n",
        "**Exercise**: Implement a multi-class classifier and tune hyperparameter. (15%)\n",
        "\n",
        "**Instruction**:\n",
        "*   Use the functions you had previously written.\n",
        "*   Preprocess the data to match the correct input format.\n",
        "*   Use mini-batch gradient descent to train the model.\n",
        "\n",
        "**Hint**:\n",
        "For data preprocessing, please be careful with the dimension of the inputs (X and y) and also note that the values of images are usually integers that fall between 0 and 255. You need to change the data type into float and scale the values between 0 and 1.\n",
        "\n",
        "In Batch Gradient Descent, we consider all the samples for every step of Gradient Descent. But what if our dataset is huge? MNIST training data contains 60000 training samples, then to take one step, the model will have to calculate the gradients of all the 60000 samples. This does not seem an efficient way. Hence, mini-batch gradient descent is recommended to be used in this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVSfqnXqXGdC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "702d6728-e89f-4b90-f90a-de9d58be9d8a"
      },
      "source": [
        "# load data\n",
        "data = np.load(\"advanced_data.npz\")\n",
        "X_train = data[\"X_train\"]\n",
        "y_train = data[\"y_train\"].reshape(-1)\n",
        "X_test = data[\"X_test\"]\n",
        "\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
        "print('Test: X=%s' % (X_test.shape, ))\n",
        "# plot first few images\n",
        "for i in range(9):\n",
        "\t# define subplot\n",
        "\tplt.subplot(330 + 1 + i)\n",
        "\t# plot raw pixel data\n",
        "\tplt.imshow(X_train[i], cmap='gray', vmin=0, vmax=255)\n",
        "# show the figure\n",
        "plt.show()\n",
        "\n",
        "# GRADED CODE: multi-class classification (Data preprocessing)\n",
        "### START CODE HERE ###\n",
        "X_train=X_train.astype('float64')\n",
        "X_test=X_test.astype('float64')\n",
        "for i in range(0, len(X_train)):\n",
        "\tX_train[i]=(X_train[i]-np.min(X_train[i]))/(np.max(X_train[i])-np.min(X_train[i]))\n",
        "for i in range(0, len(X_test)):\n",
        "\tX_test[i]=(X_test[i]-np.min(X_test[i]))/(np.max(X_test[i])-np.min(X_test[i]))\n",
        "X_train=X_train.reshape(X_train.shape[0],-1).T\n",
        "X_test=X_test.reshape(X_test.shape[0],-1).T\n",
        "y_train=np.eye(10)[y_train.astype(int)].T\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape))\n",
        "print(\"shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_test: \" + str(X_test.shape))\n",
        "\n",
        "# GRADED CODE: multi-class classification (Data preprocessing)\n",
        "### START CODE HERE ###\n",
        "None\n",
        "### END CODE HERE ###\n",
        "\n",
        "print(\"shape of X_train: \" + str(X_train.shape))\n",
        "print(\"shape of y_train: \" + str(y_train.shape))\n",
        "print(\"shape of X_test: \" + str(X_test.shape))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X_train: (784, 60000)\n",
            "shape of y_train: (10, 60000)\n",
            "shape of X_test: (784, 10000)\n",
            "shape of X_train: (784, 60000)\n",
            "shape of y_train: (10, 60000)\n",
            "shape of X_test: (784, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljAcf2tpQDR-"
      },
      "source": [
        "#You can split training and validation set here. (Optional)\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYD-qRs7doU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "76c22879-84da-4bc6-a326-3f62cd5432db"
      },
      "source": [
        "# GRADED CODE: multi-class classification\n",
        "### START CODE HERE ###\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector, of shape (number of classes, number of examples)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "        \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((10, m))\n",
        "    \n",
        "    inc = mini_batch_size\n",
        "\n",
        "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
        "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
        "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        # (approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
        "    if m % mini_batch_size != 0:\n",
        "        #(approx. 2 lines)\n",
        "        mini_batch_X = shuffled_X[:, (k + 1) * mini_batch_size:]\n",
        "        mini_batch_Y = shuffled_Y[:, (k + 1) * mini_batch_size:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n",
        "\n",
        "\n",
        "layers_dims = [784,300,100,10]\n",
        "activation_fn = [\"relu\",\"relu\", \"sigmoid\"]\n",
        "learning_rate = 0.3\n",
        "num_iterations = 35\n",
        "batch_size = 64\n",
        "print_cost = True\n",
        "classes = 10\n",
        "costs = []                         # keep track of cost\n",
        "model = Model(layers_dims, activation_fn)\n",
        "\n",
        "# Loop (gradient descent)\n",
        "for i in range(0, num_iterations):\n",
        "    mini_batches = random_mini_batches(X_train, y_train, batch_size)\n",
        "    for batch in mini_batches:\n",
        "        x_batch, y_batch = batch\n",
        "\n",
        "        # forward\n",
        "        AL = model.forward(x_batch)\n",
        "\n",
        "        # compute cost\n",
        "        if classes == 2:\n",
        "            cost = compute_BCE_cost(AL,y_batch)\n",
        "        else:\n",
        "            cost = compute_CCE_cost(AL,y_batch)\n",
        "\n",
        "        # backward\n",
        "        dA_prev = model.backward(AL,y_batch)\n",
        "        # update\n",
        "        model.update(learning_rate)\n",
        "\n",
        "    if print_cost and i % 100 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        costs.append(cost)\n",
        "            \n",
        "# plot the cost\n",
        "plt.plot(np.squeeze(costs))\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "plt.show()\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.054573\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI92fh4JXC1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d331e6f-469b-439c-d3f6-aa5819f40196"
      },
      "source": [
        "pred_train = predict(X_train, y_train, model, 10)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9999833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjcfSU2XD3-"
      },
      "source": [
        "#You can check for your validation accuracy here. (Optional)\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHFDuq2BQ2qI"
      },
      "source": [
        "pred_test = predict(X_test, None, model, 10)\n",
        "output[\"advanced_pred_test\"] = pred_test\n",
        "output[\"advanced_layers_dims\"] = layers_dims\n",
        "output[\"advanced_activation_fn\"] = activation_fn\n",
        "advanced_model_parameters = []\n",
        "for advanced_linear in model.linear:\n",
        "  advanced_model_parameters.append(advanced_linear.parameters)\n",
        "output[\"advanced_model_parameters\"] = advanced_model_parameters"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGnS3HQeNUc"
      },
      "source": [
        "# Submit prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twMsmXbQeDL_"
      },
      "source": [
        "# sanity check\n",
        "assert(list(output.keys()) == ['linear_initialize_parameters', 'linear_forward', 'linear_backward', 'linear_update_parameters', 'sigmoid', 'relu', 'softmax', 'sigmoid_backward', 'relu_backward', 'softmax_CCE_backward', 'model_initialize_parameters', 'model_forward_sigmoid', 'model_forward_relu', 'model_forward_softmax', 'model_backward_sigmoid', 'model_backward_relu', 'model_update_parameters', 'compute_BCE_cost', 'compute_CCE_cost', 'basic_pred_val', 'basic_layers_dims', 'basic_activation_fn', 'basic_model_parameters', 'advanced_pred_test', 'advanced_layers_dims', 'advanced_activation_fn', 'advanced_model_parameters'])"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCJ0XTO_zE8A"
      },
      "source": [
        "np.save(\"output.npy\", output)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFBFUUEg1to-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007c57aa-3007-4c48-95fc-42cb5c552fcd"
      },
      "source": [
        "# sanity check\n",
        "submit = np.load(\"output.npy\", allow_pickle=True).item()\n",
        "for key, value in submit.items():\n",
        "  print(str(key) + \"： \" + str(type(value)))"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear_initialize_parameters： <class 'dict'>\n",
            "linear_forward： <class 'tuple'>\n",
            "linear_backward： <class 'tuple'>\n",
            "linear_update_parameters： <class 'dict'>\n",
            "sigmoid： <class 'tuple'>\n",
            "relu： <class 'tuple'>\n",
            "softmax： <class 'tuple'>\n",
            "sigmoid_backward： <class 'numpy.ndarray'>\n",
            "relu_backward： <class 'numpy.ndarray'>\n",
            "softmax_CCE_backward： <class 'numpy.ndarray'>\n",
            "model_initialize_parameters： <class 'tuple'>\n",
            "model_forward_sigmoid： <class 'tuple'>\n",
            "model_forward_relu： <class 'tuple'>\n",
            "model_forward_softmax： <class 'tuple'>\n",
            "model_backward_sigmoid： <class 'tuple'>\n",
            "model_backward_relu： <class 'tuple'>\n",
            "model_update_parameters： <class 'dict'>\n",
            "compute_BCE_cost： <class 'numpy.float64'>\n",
            "compute_CCE_cost： <class 'numpy.float64'>\n",
            "basic_pred_val： <class 'numpy.ndarray'>\n",
            "basic_layers_dims： <class 'list'>\n",
            "basic_activation_fn： <class 'list'>\n",
            "basic_model_parameters： <class 'list'>\n",
            "advanced_pred_test： <class 'numpy.ndarray'>\n",
            "advanced_layers_dims： <class 'list'>\n",
            "advanced_activation_fn： <class 'list'>\n",
            "advanced_model_parameters： <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQqZni7jhP0"
      },
      "source": [
        "Expected output: <br>\n",
        "<small>\n",
        "linear_initialize_parameters： <class 'dict'> <br>\n",
        "linear_forward： <class 'tuple'> <br>\n",
        "linear_backward： <class 'tuple'> <br>\n",
        "linear_update_parameters： <class 'dict'> <br>\n",
        "sigmoid： <class 'tuple'> <br>\n",
        "relu： <class 'tuple'> <br>\n",
        "softmax： <class 'tuple'> <br>\n",
        "sigmoid_backward： <class 'numpy.ndarray'> <br>\n",
        "relu_backward： <class 'numpy.ndarray'> <br>\n",
        "softmax_CCE_backward： <class 'numpy.ndarray'> <br>\n",
        "model_initialize_parameters： <class 'tuple'> <br>\n",
        "model_forward_sigmoid： <class 'tuple'> <br>\n",
        "model_forward_relu： <class 'tuple'> <br>\n",
        "model_forward_softmax： <class 'tuple'> <br>\n",
        "model_backward_sigmoid： <class 'tuple'> <br>\n",
        "model_backward_relu： <class 'tuple'> <br>\n",
        "model_update_parameters： <class 'dict'> <br>\n",
        "compute_BCE_cost： <class 'numpy.ndarray'> <br> \n",
        "compute_CCE_cost： <class 'numpy.ndarray'> <br>\n",
        "basic_pred_val： <class 'numpy.ndarray'> <br>\n",
        "basic_layers_dims： <class 'list'> <br>\n",
        "basic_activation_fn： <class 'list'> <br>\n",
        "basic_model_parameters： <class 'list'> <br>\n",
        "advanced_pred_test： <class 'numpy.ndarray'> <br>\n",
        "advanced_layers_dims： <class 'list'> <br>\n",
        "advanced_activation_fn： <class 'list'> <br>\n",
        "advanced_model_parameters： <class 'list'> <br>\n",
        "</small>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GRvMDwalE5y"
      },
      "execution_count": 174,
      "outputs": []
    }
  ]
}